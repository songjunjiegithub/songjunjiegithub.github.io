<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="编程学习之旅">
<meta property="og:type" content="website">
<meta property="og:title" content="大数据学习笔记">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="大数据学习笔记">
<meta property="og:description" content="编程学习之旅">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="songjj">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>大数据学习笔记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">大数据学习笔记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/28/MySQL45%E8%AE%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/28/MySQL45%E8%AE%B2/" class="post-title-link" itemprop="url">MySQL45讲</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-28 14:36:55 / 修改时间：15:21:31" itemprop="dateCreated datePublished" datetime="2022-04-28T14:36:55+08:00">2022-04-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          有东西被加密了, 请输入密码查看.
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/04/28/MySQL45%E8%AE%B2/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/28/java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/28/java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/" class="post-title-link" itemprop="url">java开发手册</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-28 14:24:31 / 修改时间：15:21:40" itemprop="dateCreated datePublished" datetime="2022-04-28T14:24:31+08:00">2022-04-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          有东西被加密了, 请输入密码查看.
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/04/28/java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/27/scala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/27/scala/" class="post-title-link" itemprop="url">scala</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-27 14:13:14 / 修改时间：14:17:45" itemprop="dateCreated datePublished" datetime="2022-04-27T14:13:14+08:00">2022-04-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index"><span itemprop="name">big data</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/scala/" itemprop="url" rel="index"><span itemprop="name">scala</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="复习"><a href="#复习" class="headerlink" title="复习"></a>复习</h1><p>每行语句不需要带分号;若一行有多条语句,除了最后一个,其他带分号</p>
<h1 id="三种输出形式"><a href="#三种输出形式" class="headerlink" title="三种输出形式"></a>三种输出形式</h1><p>1.+</p>
<p>2.printf  → %</p>
<p>3.s”$name”</p>
<h1 id="文档注释生成"><a href="#文档注释生成" class="headerlink" title="文档注释生成"></a>文档注释生成</h1><p>scaladoc -d d:&#x2F; Hello.scala</p>
<h1 id="类型判断"><a href="#类型判断" class="headerlink" title="类型判断"></a>类型判断</h1><p>num.isInstanceOf[Int]</p>
<p>没有形参可以省略括号</p>
<h1 id="数值体系"><a href="#数值体系" class="headerlink" title="数值体系"></a>数值体系</h1><p><img src="/2022/04/27/scala/%E5%A4%8D%E4%B9%A0%20d18977099b184a7f9f7de80a0f32a237/Untitled.png" alt="Untitled"></p>
<p>Null→实例null,可赋值给任何AnyRef类型</p>
<p>Unit→实例(),函数返回值为空</p>
<p>Nothing→没有正常返回值的方法的返回类型,通常抛出异常时</p>
<h1 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h1><p>默认Int,Double</p>
<p>float→f,F</p>
<p>Long→l,L</p>
<p>字符单引号,字符串双引号</p>
<h1 id="标识符命名"><a href="#标识符命名" class="headerlink" title="标识符命名"></a>标识符命名</h1><p>首字符为操作符(+-等)后续也需跟操作符,不能在中间或最后.</p>
<p>&#96;&#96;包括可以为关键字等.</p>
<p>不能命名为_</p>
<h1 id="运算符等"><a href="#运算符等" class="headerlink" title="运算符等"></a>运算符等</h1><p>整数&#x2F;保留整数部分</p>
<p>a%b&#x3D;a-a&#x2F;b*b </p>
<p>如果b是负数,一般用(a%b+b)%b</p>
<p>不支持三目,使用val num &#x3D; if(5&gt;4) 5 else 4</p>
<h1 id="键盘输入语句"><a href="#键盘输入语句" class="headerlink" title="键盘输入语句"></a>键盘输入语句</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import scala.io.StdIn</span><br><span class="line">StdIn.readLine()</span><br><span class="line">StdIn.readInt()</span><br><span class="line">StdIn.readDouble()</span><br></pre></td></tr></table></figure>

<h1 id="for"><a href="#for" class="headerlink" title="for"></a>for</h1><p>for (i  ←1 to 3)      [1,3]</p>
<p>for (i ← 1 until 3)    [1,3)</p>
<p>控制步长for(i &lt;- Range(1,3,2) 或循环守卫</p>
<p>for (i ← List(1,2,3))</p>
<p>for (i ← 1 to 3 if i ≠ 2)</p>
<p>for(i ← 1 to 3;j&#x3D;4-i)</p>
<p>for(i &lt;- 1 to 3; j &lt;- 1 to 3)或</p>
<p>for {</p>
<p>i ← 1 to 3</p>
<p>j ← 1 to 3}</p>
<p>val res &#x3D; for(i &lt;- 1 to 10) yield i            &#x2F;将遍历过程中处理的结果返回到一个新 Vector 集合中</p>
<p>if有返回值,while无返回值</p>
<h1 id="break和continue"><a href="#break和continue" class="headerlink" title="break和continue"></a>break和continue</h1><p>breakable()函数，当传入的是代码块的时候，一般会将小括号改成大括号。</p>
<p>breakable {</p>
<p>while (n&lt;20) {</p>
<p>n +&#x3D; 1</p>
<p>if (n &#x3D;&#x3D; 18) break()</p>
<p>}</p>
<p>}</p>
<p>循环守卫可代替continue</p>
<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><p>函数几乎等同于方法，创建不依赖于类或对象</p>
<p>函数也当作数据类型，可以当作输入（参数）和输出（返回值）</p>
<p>def funName([parameter:type],…)[[:resultType]&#x3D;]</p>
<p>{</p>
<p>code</p>
<p>return result</p>
<p>}</p>
<p>无[[:resultType]&#x3D;]则return不生效，最后一行的结果作为返回值。有result必须要[[:resultType]&#x3D;]，否则返回()</p>
<p>任何语法结构可以嵌套。函数中再声明&#x2F;定义函数，方法中再声明&#x2F;定义方法，类中再声明&#x2F;定义类。</p>
<p>def say(name:String&#x3D;”jack”,age:String&#x3D;”27”)      参数默认值</p>
<p>调用时使用可以代名参数say(name&#x3D;”jack”,age&#x3D;”27”)</p>
<p>递归函数必须要有返回值类型</p>
<h2 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h2><p>要写在行参列表后</p>
<p>def sum(args:Int*)      0到多个参数</p>
<p>def sum(n1:Int,args:Int*)    1到多个参数</p>
<p>若返回类型为Unit则称函数为过程。有&#x3D;号时可以根据最后一行自动推断，实际上有返回值，不是过程。</p>
<h2 id="惰性函数"><a href="#惰性函数" class="headerlink" title="惰性函数"></a>惰性函数</h2><p>lazy val res&#x3D;sum(10,20),延迟执行，直到需要取值的时候</p>
<p>不能修饰var。</p>
<h1 id="try"><a href="#try" class="headerlink" title="try"></a>try</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	<span class="keyword">var</span> r = <span class="number">10</span> / <span class="number">0</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> ex:<span class="type">ArithmeticException</span> =&gt; &#123;println(<span class="string">&quot;&quot;</span>)&#125;</span><br><span class="line">	<span class="keyword">case</span> ex:<span class="type">Exception</span> =&gt; &#123;println(<span class="string">&quot;&quot;</span>)&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span>&#123;&#125;</span><br><span class="line"><span class="comment">// throw有返回值类型:Nothing</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span></span>():<span class="type">Nothing</span>=&#123;</span><br><span class="line">	<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ArithmeticException</span>(<span class="string">&quot;算术异常&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h1><p>类变量：val age:Int&#x3D;_会创建默认值0</p>
<p>val age:Int    .class文件生成两个方法 age()类似getter(),age_$eq(int x$1)类似setter()</p>
<p>类默认为public,一个scala文件可定义多个类</p>
<p>[修饰符] class ClassName {</p>
<p>[访问修饰符]var 属性名称 [：类型] &#x3D; 属性值 &#x2F;&#x2F;赋值为null一定加类型，否则为Null类型，val age:Int&#x3D;_会创建默认值0，val也可。类型可以忽略，当使用继承和多态，子类对象交给父类引用时，就要加上类型。</p>
<p>&#x2F;&#x2F; 方法定义和函数类似</p>
<p>}</p>
<h2 id="构造器"><a href="#构造器" class="headerlink" title="构造器"></a>构造器</h2><p><strong>主构造器会执行类定义中的所有语句,如果主构造器无参数，小括号可省略.若想主构造器私有，可以class</strong> 类名 private(形参列表).</p>
<p>主参数：若无var&#x2F;val，则为一个局部变量,val则为一个私有的只读类属性，var则为一个私有的可读写类属性，提供对应的xxx()[类似 getter]&#x2F;xxx_$eq()[类似 setter]方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span>(<span class="params">形参列表</span>) </span>&#123;	<span class="comment">// 主构造器</span></span><br><span class="line"><span class="comment">// 类体</span></span><br><span class="line"><span class="function"><span class="keyword">def</span>	<span class="title">this</span></span>(形参列表) &#123;	<span class="comment">// 辅助构造器</span></span><br><span class="line">	<span class="comment">//辅助构造器，必须在第一行显式调用主构造器(可以是直接，也可以是间接)</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span>	<span class="title">this</span></span>(形参列表) &#123;	<span class="comment">//辅助构造器可以有多个...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="生成getXxx（）和-setXxx（）方法"><a href="#生成getXxx（）和-setXxx（）方法" class="headerlink" title="生成getXxx（）和 setXxx（）方法"></a>生成getXxx（）和 setXxx（）方法</h2><p>属性加入@BeanPropetry 注解后，会生成 getXXX 和 setXXX 的方法和xxx(),xxx_$eq()方法，没有冲突，二者可以共存。</p>
<h1 id="包"><a href="#包" class="headerlink" title="包"></a>包</h1><p>protected→子类可以，同包类不行,默认→public,private→类和伴生对象。protected[visit]表示在visit包(包括子包)下也可以使用</p>
<p>不能使用关键字</p>
<p>package{}中可以创建包，类，特质，object等。</p>
<p>子包可以直接使用父包的类(java需import)，若有重名的多个类，使用就近原则，父包使用子包需import导入</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.flow &#123;</span><br><span class="line">	<span class="class"><span class="keyword">class</span> <span class="title">User</span></span>&#123;<span class="comment">//com.flow.User</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">package</span> scala2&#123;<span class="comment">//package com.flow.scala2</span></span><br><span class="line">		<span class="class"><span class="keyword">class</span> <span class="title">User</span></span>&#123;<span class="comment">//com.flow.scala2.User</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>java包不能包含函数&#x2F;方法或变量定义，scala包对象可以</p>
<p>包对象要和子包名一样，就可以在子包中使用,每个包可有一个包对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.flow&#123;</span><br><span class="line">	<span class="keyword">package</span> <span class="class"><span class="keyword">object</span> <span class="title">scala</span></span>&#123;</span><br><span class="line">		<span class="keyword">var</span> name = <span class="string">&quot;king&quot;</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">sayHi</span></span>():<span class="type">Unit</span>=&#123;&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">package</span> scala &#123;</span><br><span class="line">		<span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">			<span class="function"><span class="keyword">def</span> <span class="title">test</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line">				println(name)</span><br><span class="line">				sayHi()</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>自动引入的包，java.lang包,scala包,Predef包.import可以出现在任何地方,作用到语句块尾部。</p>
<p>._全部类</p>
<p>.{ClassName1,,,}指定类</p>
<p>.{ClassName1⇒RenameClassName1,…}类重命名</p>
<p>.{ClassName1⇒_,…}隐藏类.比如import java.util.{ HashMap&#x3D;&gt;_, _}表示引入除HashMap外的所有类</p>
<h1 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h1><p>class 子类名 extends 父类名 {}</p>
<p>子类继承父类所有属性，私有属性不能直接访问，可通过公共方法访问</p>
<h1 id="重写方法"><a href="#重写方法" class="headerlink" title="重写方法"></a>重写方法</h1><p>override修饰,super调用超类方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> <span class="keyword">extends</span> <span class="title">B</span> </span>&#123;</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>() &#123;</span><br><span class="line">		<span class="keyword">super</span>.f()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="类型检查和转换"><a href="#类型检查和转换" class="headerlink" title="类型检查和转换"></a>类型检查和转换</h1><p>classOf[String]→类似java的String.class</p>
<p>obj.isInstanceOf[T]→obj instanceof T 判断 obj 是不是 T 类型</p>
<p>obj.asInstanceOf[T]→ Java 的(T)obj 将 obj 强转成 T 类型</p>
<p>只有主构造器可以调用父类的构造器。辅助构造器不能直接调用父类的构造器。</p>
<h2 id="覆写字段"><a href="#覆写字段" class="headerlink" title="覆写字段"></a>覆写字段</h2><p><strong>子类改写父类的字段，我们称为覆写&#x2F;重写字段</strong></p>
<p>def只能重写另一个 def(即：方法只能重写另一个方法)</p>
<p>val 只能重写另一个 val 属性 或 重写不带参数的 def</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;def <span class="title function_">sal</span>():<span class="title class_">Int</span>=&#123;<span class="keyword">return</span> <span class="number">10</span>&#125;&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> <span class="keyword">extends</span> <span class="title class_ inherited__">A</span>&#123;override val <span class="attr">sal</span>:<span class="title class_">Int</span>=<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure>

<p>var只能重写另一个抽象的 var属性</p>
<h2 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">	<span class="keyword">var</span> name : <span class="type">String</span> <span class="comment">//抽象的字段</span></span><br><span class="line">	<span class="keyword">var</span> age : <span class="type">Int</span> <span class="comment">//  抽象的字段</span></span><br><span class="line">	<span class="keyword">var</span> color : <span class="type">String</span> = <span class="string">&quot;black&quot;</span> <span class="comment">//普通属性</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">cry</span></span>() <span class="comment">//抽象方法,不需要标记 abstract,可以没有抽象方法</span></span><br><span class="line">	<span class="comment">//在抽象类中可以有实现的方法</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">sayHi</span> </span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">		println(<span class="string">&quot;xxx&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.notion.so/c34a94a12cd84918abcd134f5e9ef2b8">https://www.notion.so/c34a94a12cd84918abcd134f5e9ef2b8</a></p>
<h2 id="伴生对象"><a href="#伴生对象" class="headerlink" title="伴生对象"></a>伴生对象</h2><p>伴生类和伴生对象均可单独存在，当一个文件存在伴生类和对象时，idea中文件图标会发生变化。</p>
<p><a target="_blank" rel="noopener" href="https://www.notion.so/4940ed9a1f33413d96a2f422e8046034">https://www.notion.so/4940ed9a1f33413d96a2f422e8046034</a> </p>
<h3 id="apply方法"><a href="#apply方法" class="headerlink" title="apply方法"></a>apply方法</h3><p>当使用类名(参数)时候会调用类名.apply(参数)</p>
<p>使用类的实例(参数时)会调用伴生类中apply</p>
<h1 id="trait"><a href="#trait" class="headerlink" title="trait"></a>trait</h1><p>trait等价于(interface+abstract class)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">特质名</span></span>&#123;</span><br><span class="line">	特质体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>java中的接口可以当做特质使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span> <span class="keyword">extends</span> <span class="title">特质1</span> <span class="keyword">with</span> <span class="title">特质2</span> <span class="keyword">with</span> <span class="title">特质3</span> ...</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span> <span class="keyword">extends</span> <span class="title">父类名</span> <span class="keyword">with</span> <span class="title">特质1</span> <span class="keyword">with</span> <span class="title">特质2</span> <span class="keyword">with</span> <span class="title">特质3</span> ...</span></span><br></pre></td></tr></table></figure>

<p>特质可以同时拥有抽象方法和具体方法，一个类可以实现&#x2F;继承多个特质</p>
<h2 id="带有特质的对象，动态混入"><a href="#带有特质的对象，动态混入" class="headerlink" title="带有特质的对象，动态混入"></a>带有特质的对象，动态混入</h2><p>扩展目标类的功能</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">O</span></span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(id:<span class="type">Int</span>) &#123;println(id)&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span></span>&#123;&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">C</span></span>&#123;</span><br><span class="line">	<span class="keyword">val</span> a = <span class="keyword">new</span> <span class="type">A</span> <span class="keyword">with</span> <span class="type">O</span></span><br><span class="line">	a.insert(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">D</span></span>&#123;&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">E</span></span>&#123;</span><br><span class="line">	<span class="keyword">val</span> d = <span class="keyword">new</span> <span class="type">D</span> <span class="keyword">with</span> <span class="type">O</span></span><br><span class="line">	d.insert(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">F</span></span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">say</span></span>()</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">G</span></span>&#123;</span><br><span class="line">	<span class="keyword">val</span> f = <span class="keyword">new</span> <span class="type">F</span> <span class="keyword">with</span> <span class="type">O</span>&#123;</span><br><span class="line">		<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>():<span class="type">Unit</span>=&#123;println(<span class="string">&quot;say&quot;</span>)&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	f.say()</span><br><span class="line">	f.insert(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="叠加特质"><a href="#叠加特质" class="headerlink" title="叠加特质"></a>叠加特质</h2><p>构建对象时混入多个特质</p>
<p>特质声明顺序从左到右，方法执行顺序从右到左</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.chapter08.mixin</span><br><span class="line"></span><br><span class="line"><span class="comment">//看看混入多个特质的特点(叠加特质) </span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AddTraits</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">//说明</span></span><br><span class="line">	<span class="comment">//1. 创建 MySQL4 实例时，动态的混入 DB4 和 File4</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//研究第一个问题，当我们创建一个动态混入对象时，其顺序是怎样的</span></span><br><span class="line">	<span class="comment">//总结一句话</span></span><br><span class="line">	<span class="comment">//Scala 在叠加特质的时候，会首先从后面的特质开始执行(即从左到右)</span></span><br><span class="line">	<span class="comment">//1.Operate4...</span></span><br><span class="line">	<span class="comment">//2.Data4</span></span><br><span class="line">	<span class="comment">//3.DB4</span></span><br><span class="line">	<span class="comment">//4.File4</span></span><br><span class="line">	<span class="keyword">val</span> mysql = <span class="keyword">new</span> <span class="type">MySQL4</span> <span class="keyword">with</span> <span class="type">DB4</span> <span class="keyword">with</span> <span class="type">File4</span> </span><br><span class="line">	println(mysql)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//研究第 2 个问题，当我们执行一个动态混入对象的方法，其执行顺序是怎样的</span></span><br><span class="line">	<span class="comment">//顺序是，(1)从右到左开始执行 , (2)当执行到 super 时，是指的左边的特质 (3) 如果左边没有特质了，则 super 就是父特质</span></span><br><span class="line">	<span class="comment">//1. 向文件&quot;</span></span><br><span class="line">	<span class="comment">//2. 向数据库</span></span><br><span class="line">	<span class="comment">//3. 插入数据 100 mysql.insert(100)</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Operate4</span> </span>&#123; <span class="comment">//特点println(&quot;Operate4...&quot;)</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(id: <span class="type">Int</span>) <span class="comment">//抽象方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Data4</span> <span class="keyword">extends</span> <span class="title">Operate4</span> </span>&#123; <span class="comment">//特质，继承了 Operate4 println(&quot;Data4&quot;)</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(id: <span class="type">Int</span>): <span class="type">Unit</span> = &#123; <span class="comment">//实现/重写 Operate4 的 insert</span></span><br><span class="line">		println(<span class="string">&quot;插入数据 = &quot;</span> + id)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DB4</span> <span class="keyword">extends</span> <span class="title">Data4</span> </span>&#123; <span class="comment">//特质，继承 Data4 println(&quot;DB4&quot;)</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(id: <span class="type">Int</span>): <span class="type">Unit</span> = &#123; <span class="comment">// 重写 Data4 的 insert</span></span><br><span class="line">		println(<span class="string">&quot;向数据库&quot;</span>) <span class="keyword">super</span>.insert(id)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">File4</span> <span class="keyword">extends</span> <span class="title">Data4</span> </span>&#123; <span class="comment">//特质，继承 Data4 println(&quot;File4&quot;)</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(id: <span class="type">Int</span>): <span class="type">Unit</span> = &#123; <span class="comment">// 重写 Data4 的 insert</span></span><br><span class="line">		println(<span class="string">&quot;向文件&quot;</span>)</span><br><span class="line">		<span class="keyword">super</span>.insert(id) <span class="comment">//调用了 insert 方法(难点)，这里 super 在动态混入时，不一定是父类</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL4</span>	</span>&#123;&#125; <span class="comment">//普通类</span></span><br></pre></td></tr></table></figure>

<p>叠加特质注意事项和细节</p>
<p>特质声明顺序从左到右。</p>
<p>Scala在执行叠加对象的方法时，会首先从后面的特质(从右向左)开始执行</p>
<p>Scala中特质中如果调用 super，并不是表示调用父特质的方法，而是向前面（左边）继续查找特质，如果找不到，才会去父特质查找</p>
<p>如果想要调用具体特质的方法，可以指定：<strong>super[特质].xxx(…).其中的泛型必须是该特质的直接超类类</strong>型</p>
<p>富接口：即该特质中既<strong>有抽象方法</strong>，又有<strong>非抽象方</strong>法</p>
<p>特质中可以定义具体字段，如果初始化了就是具体字段，如果不初始化就是抽象字段。混入该特质的类就具有了该字段，字段不是继承，而是直接加入类，成为自己的字段</p>
<p>特质中未被初始化的字段在具体的子类中必须被重写。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.chapter08.mixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MixInPro</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> mySQL = <span class="keyword">new</span> <span class="type">MySQL6</span> <span class="keyword">with</span> <span class="type">DB6</span> &#123; <span class="keyword">override</span> <span class="keyword">var</span> sal = <span class="string">&quot;&quot;</span>&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DB6</span>	</span>&#123;</span><br><span class="line">	<span class="keyword">var</span> sal:<span class="type">Int</span> <span class="comment">//抽象字段</span></span><br><span class="line">	<span class="keyword">var</span> opertype : <span class="type">String</span> = <span class="string">&quot;insert&quot;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL6</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="特质的构造顺序"><a href="#特质的构造顺序" class="headerlink" title="特质的构造顺序"></a>特质的构造顺序</h2><h2 id="扩展类的特质"><a href="#扩展类的特质" class="headerlink" title="扩展类的特质"></a>扩展类的特质</h2><p>特质可以继承类，以用来拓展该特质的一些功能</p>
<p>所有混入该特质的类，会自动成为那个特质所继承的超类的子类</p>
<p>如果混入该特质的类，已经继承了另一个类(A 类)，则要求 A 类是特质超类的子类，否则就会出现了多继承现象，发生错误</p>
<h2 id="自身类型"><a href="#自身类型" class="headerlink" title="自身类型"></a>自身类型</h2><p>自身类型：主要是为了解决特质的循环依赖问题，同时可以确保特质在不扩展某个类的情况下，依然可以做到限制混入该特质的类的类型。</p>
<h2 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span></span>&#123;</span><br><span class="line">	<span class="class"><span class="keyword">class</span> <span class="title">B</span></span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">A</span></span>&#123;</span><br><span class="line">	<span class="class"><span class="keyword">class</span> <span class="title">C</span></span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">	<span class="keyword">val</span> a:<span class="type">A</span> = <span class="keyword">new</span> <span class="type">A</span>()</span><br><span class="line">	<span class="keyword">val</span> b = <span class="keyword">new</span> a.<span class="type">B</span>()</span><br><span class="line">	<span class="keyword">val</span> c = <span class="keyword">new</span> <span class="type">A</span>.<span class="type">C</span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>内部类访问外部类属性：</p>
<p><strong>外部类名.this.属性</strong>名</p>
<p><strong>外部类别名访问             外部类名别名.属性名</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScalaOuterClass</span> </span>&#123;</span><br><span class="line">	myouter =&gt; <span class="comment">//这里我们可以这里理解 外部类的别名 看做是外部类的一个实例</span></span><br><span class="line">	<span class="class"><span class="keyword">class</span> <span class="title">ScalaInnerClass</span> </span>&#123; <span class="comment">//成员内部类,</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">info</span></span>() = &#123;</span><br><span class="line">			<span class="comment">// 访问方式：外部类别名.属性名</span></span><br><span class="line">			<span class="comment">// 只是这种写法比较特别，学习 java 的同学可能更容易理解 ScalaOuterClass.class 的写法. </span></span><br><span class="line">			println(<span class="string">&quot;name~ = &quot;</span> + myouter.name + <span class="string">&quot; sal~ =&quot;</span> + myouter.sal)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//定义两个属性</span></span><br><span class="line">	<span class="keyword">var</span> name = <span class="string">&quot;jack&quot;</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">var</span> sal = <span class="number">800.9</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaOuterClass</span> </span>&#123; <span class="comment">//伴生对象</span></span><br><span class="line">	<span class="class"><span class="keyword">class</span> <span class="title">ScalaStaticInnerClass</span> </span>&#123; <span class="comment">//静态内部类</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="类型投影"><a href="#类型投影" class="headerlink" title="类型投影"></a>类型投影</h2><p>在方法声明上，如果使用 外部类#内部类 的方式，表示忽略内部类的对象关系， 等同于 Java 中内部类的语法操作，我们将这种方式称之为 类型投影（即：忽略对象的创建方式， 只考虑类型）</p>
<h2 id="隐式转换和隐式值"><a href="#隐式转换和隐式值" class="headerlink" title="隐式转换和隐式值"></a>隐式转换和隐式值</h2><p>implicit</p>
<h2 id="隐式函数"><a href="#隐式函数" class="headerlink" title="隐式函数"></a>隐式函数</h2><p>隐式转换与函数名无关，只与函数签名(函数参数类型与返回值类型)有关。</p>
<p>需保证当前环境只有一个隐式函数能被识别</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> </span>&#123;&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">c</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line">		println(<span class="string">&quot;ssss&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">s</span></span>(a:<span class="type">A</span>):<span class="type">B</span>=&#123;</span><br><span class="line">	<span class="keyword">new</span> <span class="type">B</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span>=&#123;</span><br><span class="line">	(<span class="keyword">new</span> <span class="type">A</span>).c</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="隐式值"><a href="#隐式值" class="headerlink" title="隐式值"></a>隐式值</h2><p>implicit val a:String&#x3D;”sss”,当隐式参数缺省时，会找到对应类型的隐式值传进来 </p>
<p>传值&gt;隐式值&gt;默认值，都没有则会报错</p>
<h2 id="隐式类"><a href="#隐式类" class="headerlink" title="隐式类"></a>隐式类</h2><p>隐式类构造参数有且只能有一个。不能是顶级的。不能是case class。不能有与之同名称的标识符。</p>
<h2 id="转换时机"><a href="#转换时机" class="headerlink" title="转换时机"></a>转换时机</h2><p>参数类型与目标参数不一样。不存在的方法或成员变量。</p>
<p>隐式操作不能嵌套使用。</p>
<h1 id="数据结构→先跳过"><a href="#数据结构→先跳过" class="headerlink" title="数据结构→先跳过"></a>数据结构→先跳过</h1><h1 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h1><h2 id="match"><a href="#match" class="headerlink" title="match"></a>match</h2><p>以case _ ⇒结尾</p>
<p>如果想匹配范围用条件守卫，case _ if   其中_不表示默认匹配，表示忽略输入.</p>
<p>模式匹配有返回值</p>
<p>case后跟变量，则match前表达式值会赋给此变量</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ch = &#x27;a&#x27;</span><br><span class="line">ch <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &#x27;+&#x27; =&gt; println(&#x27;+&#x27;)</span><br><span class="line">    <span class="keyword">case</span> &#x27;-&#x27; =&gt; println(&#x27;-&#x27;)</span><br><span class="line">    <span class="keyword">case</span> _ <span class="keyword">if</span> (ch.equals(&#x27;a&#x27;)) =&gt; println(&#x27;a&#x27;)</span><br><span class="line">    <span class="keyword">case</span> b =&gt; println(b)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;not match&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="类型匹配"><a href="#类型匹配" class="headerlink" title="类型匹配"></a>类型匹配</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">obj <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> a:<span class="type">Int</span> =&gt; println(<span class="string">s&quot;<span class="subst">$a</span> is Int&quot;</span>) <span class="comment">// 相当于a=obj,再判断是否是Int类型</span></span><br><span class="line">            <span class="keyword">case</span> b:<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>] =&gt; println(<span class="string">&quot;对象是map集合&quot;</span>)</span><br><span class="line">            <span class="keyword">case</span> c:<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] =&gt; println(<span class="string">&quot;,,,&quot;</span>)</span><br><span class="line">            <span class="keyword">case</span> d:<span class="type">Array</span>[<span class="type">String</span>] =&gt; println(d)</span><br><span class="line">            <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;nothing&quot;</span>)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配数组</span></span><br><span class="line"><span class="keyword">val</span> arrs = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">0</span>),<span class="type">Array</span>(<span class="number">1</span>,<span class="number">0</span>),<span class="type">Array</span>(<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>),<span class="type">Array</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),<span class="type">Array</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> (arr &lt;- arrs) &#123;</span><br><span class="line">    <span class="keyword">val</span> res = arr <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; <span class="string">&quot;0&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(x, y) =&gt; x + <span class="string">&quot;=&quot;</span> + y</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(x, _*) =&gt; x</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    println(res)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 匹配列表</span></span><br><span class="line"><span class="keyword">val</span> lists = <span class="type">Array</span>(<span class="type">List</span>(<span class="number">0</span>), <span class="type">List</span>(<span class="number">1</span>,<span class="number">0</span>), <span class="type">List</span>(<span class="number">88</span>), <span class="type">List</span>(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="type">List</span>(<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"><span class="keyword">for</span> (lis &lt;- lists) &#123;</span><br><span class="line">    <span class="keyword">val</span> res = lis <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span> :: <span class="type">Nil</span> =&gt; <span class="string">&quot;0&quot;</span></span><br><span class="line">        <span class="keyword">case</span> x :: y :: <span class="type">Nil</span> =&gt; <span class="string">&quot;两个元素&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span> :: tail =&gt; <span class="string">&quot;开头为1&quot;</span></span><br><span class="line">        <span class="keyword">case</span> x :: <span class="type">Nil</span> =&gt; x</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">&quot;something else&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配元素</span></span><br><span class="line">(<span class="number">0</span>,_),(x,y),(y,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h2 id="对象匹配"><a href="#对象匹配" class="headerlink" title="对象匹配"></a>对象匹配</h2><p>会调用unapply方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将obj传给ClassName的unapply方法,返回值为Some()或None,</span></span><br><span class="line"><span class="comment">// 为None则跳过，否则将Some的值赋值给a</span></span><br><span class="line"><span class="comment">// unapply的返回值个数要和case调用里的参数个数一样</span></span><br><span class="line">obj <span class="keyword">match</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="type">ClassName</span>(a,b,c) =&gt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Names</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">unapply</span></span>(str:<span class="type">String</span>):<span class="type">Option</span>[<span class="type">Seq</span>[<span class="type">String</span>]]=&#123;</span><br><span class="line">		<span class="keyword">if</span> (str.contains(<span class="string">&quot;,&quot;</span>) &#123;</span><br><span class="line">			<span class="type">Some</span>(str.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="type">None</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="变量声明中的模式"><a href="#变量声明中的模式" class="headerlink" title="变量声明中的模式"></a>变量声明中的模式</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (a,b,c)=(<span class="number">1</span>,<span class="number">2</span>,<span class="string">&quot;h&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> (q,r)=<span class="type">BigInt</span>(<span class="number">10</span>)/%<span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(first,second,_*)=<span class="type">Array</span>(<span class="number">1</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">9</span>)</span><br></pre></td></tr></table></figure>

<h2 id="for循环中"><a href="#for循环中" class="headerlink" title="for循环中"></a>for循环中</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map = <span class="type">Map</span>(<span class="string">&quot;A&quot;</span>-&gt;<span class="number">1</span>,<span class="string">&quot;B&quot;</span>-&gt;<span class="number">2</span>,<span class="string">&quot;C&quot;</span>-&gt;<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> ((k,v) &lt;- map) println(k + <span class="string">&quot;-&gt;&quot;</span> + v)</span><br><span class="line"><span class="keyword">for</span> ((k,<span class="number">0</span>) &lt;- map) println(k)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="样例类"><a href="#样例类" class="headerlink" title="样例类"></a>样例类</h2><p>使用case声明，默认每个参数都为val，提供apply，unapply，toString,equals,hashCode,copy</p>
<h2 id="中置表达式"><a href="#中置表达式" class="headerlink" title="中置表达式"></a>中置表达式</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">9</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> first::second::rest =&gt; </span><br><span class="line">	<span class="keyword">case</span> _ =&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="密封类"><a href="#密封类" class="headerlink" title="密封类"></a>密封类</h2><p>sealed</p>
<h1 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h1><h2 id="偏函数"><a href="#偏函数" class="headerlink" title="偏函数"></a>偏函数</h2><h2 id="作为参数的函数"><a href="#作为参数的函数" class="headerlink" title="作为参数的函数"></a>作为参数的函数</h2><p>:(参数类型)⇒返回类型</p>
<p>scala中函数也是有类型的</p>
<h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><p>不写def,&#x3D;变⇒,不写return类型推导</p>
<p>(x:Double)⇒{3*x} </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> triple = (x:<span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">	<span class="number">3</span> * x</span><br><span class="line">&#125; </span><br><span class="line">println(triple(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<h2 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h2><p>接收函数作为参数的函数。</p>
<p>返回类型可以为函数类型,此时可以分步执行也可以一步执行</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minusxy</span></span>(x:<span class="type">Int</span>)=&#123;</span><br><span class="line">	(y:<span class="type">Int</span>) =&gt; x - y</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> f1 = minusxy(<span class="number">3</span>)</span><br><span class="line">println(f1(<span class="number">9</span>))</span><br><span class="line"></span><br><span class="line">println(minusxy(<span class="number">3</span>)(<span class="number">9</span>))</span><br></pre></td></tr></table></figure>

<h2 id="参数-类型-推断"><a href="#参数-类型-推断" class="headerlink" title="参数(类型)推断"></a>参数(类型)推断</h2><p>参数类型可以推断时，可以省略参数类型</p>
<p>当传入的函数只有单个参数时，可以省略括号</p>
<p>如果变量只在⇒后出现一次，可以用_来代替</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list.map((x:<span class="type">Int</span>)=&gt;x+<span class="number">1</span>)</span><br><span class="line">list.map((x)=&gt;x+<span class="number">1</span>)</span><br><span class="line">list.map(x=&gt;x+<span class="number">1</span>)</span><br><span class="line">list.map(_+<span class="number">1</span>)</span><br><span class="line">list.reduce(_+_)</span><br></pre></td></tr></table></figure>

<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><h2 id="函数柯里化"><a href="#函数柯里化" class="headerlink" title="函数柯里化"></a>函数柯里化</h2><p>接收多个参数的函数转化为接收单个参数的函数的过程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x:<span class="type">Int</span>,y:<span class="type">Int</span>)=x*y</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mulCurry</span></span>(x:<span class="type">Int</span>)=(y:<span class="type">Int</span>)=&gt;x*y</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mulCurry2</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>)=x*y</span><br></pre></td></tr></table></figure>

<h2 id="控制抽象"><a href="#控制抽象" class="headerlink" title="控制抽象"></a>控制抽象</h2><p>控制抽象函数，参数是函数，函数参数既没有输入值又没有输出值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(f1:=&gt;<span class="type">Unit</span>)=&#123;</span><br><span class="line">	f1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">f&#123;</span><br><span class="line">	println(<span class="string">&quot;start&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/27/git/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/27/git/" class="post-title-link" itemprop="url">git</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-27 14:12:36 / 修改时间：14:15:57" itemprop="dateCreated datePublished" datetime="2022-04-27T14:12:36+08:00">2022-04-27</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="git"><a href="#git" class="headerlink" title="git"></a>git</h1><p><a target="_blank" rel="noopener" href="https://www.liaoxuefeng.com/wiki/896043488029600/1216289527823648">https://www.liaoxuefeng.com/wiki/896043488029600/1216289527823648</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hudashi/article/details/7664631">git rebase简介(基本篇)_Robin Hu的专栏-CSDN博客_git rebase</a></p>
<h1 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h1><p>把目录变成git可以管理的仓库</p>
<p>所有的版本控制系统，其实只能跟踪文本文件的改动，word的格式是二进制的，版本控制系统是没法跟踪Word文件的改动。</p>
<p>不要使用Windows自带的<strong>记事本</strong>编辑任何文本文件。原因是Microsoft开发记事本的团队使用了一个非常弱智的行为来保存UTF-8编码的文件，他们自作聪明地在每个文件开头添加了0xefbbbf（十六进制）的字符。</p>
<h1 id="git-add"><a href="#git-add" class="headerlink" title="git add"></a>git add</h1><p>git add提交的是修改,如果修改不git add就不会被提交到分支中</p>
<h1 id="git-commit"><a href="#git-commit" class="headerlink" title="git commit"></a>git commit</h1><p>-m:提交信息 </p>
<h1 id="git-statuts"><a href="#git-statuts" class="headerlink" title="git statuts"></a>git statuts</h1><p>当前状态。显示哪些⽂件已被staged、未被staged以及未跟踪(untracked)。</p>
<h1 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h1><p>比较工作区和暂存区。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git diff readme.txt </span><br><span class="line">diff --git a/readme.txt b/readme.txt</span><br><span class="line">index 46d49bf..9247db6 100644</span><br><span class="line">--- a/readme.txt</span><br><span class="line">+++ b/readme.txt</span><br><span class="line">@@ -1,2 +1,2 @@</span><br><span class="line">-Git is a version control system.</span><br><span class="line">+Git is a distributed version control system.</span><br><span class="line"> Git is free software.</span><br></pre></td></tr></table></figure>

<p>git diff HEAD</p>
<p>⽐较⼯作区和上⼀次commit后的修改</p>
<p>git diff –cached</p>
<p>⽐较暂存区和上⼀次commit后的修改</p>
<h1 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h1><p>显示从最近到最远的提交日志</p>
<p>–pretty&#x3D;oneline:每个提交打印成一行 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --graph --pretty=oneline --abbrev-commit</span><br></pre></td></tr></table></figure>

<h1 id="HEAD"><a href="#HEAD" class="headerlink" title="HEAD"></a>HEAD</h1><p>表示当前版本</p>
<p>HEAD^表示上一个版本</p>
<p>HEAD^^表示上上一个版本</p>
<p>HEAD~100表示上100个版本</p>
<h1 id="git-reset-–hard-commit-id"><a href="#git-reset-–hard-commit-id" class="headerlink" title="git reset –hard commit_id"></a>git reset –hard commit_id</h1><p>回到某个版本</p>
<p>git reset —hard HEAD^回到上一个版本</p>
<p>若不记得最新版本的commitid,可用reflog</p>
<h1 id="git-reflog"><a href="#git-reflog" class="headerlink" title="git reflog"></a>git reflog</h1><p>显示本地所有的commit日志</p>
<h1 id="工作区-暂存区-版本库"><a href="#工作区-暂存区-版本库" class="headerlink" title="工作区,暂存区,版本库"></a>工作区,暂存区,版本库</h1><p>工作区:目录下的文件 </p>
<p>版本库:.git </p>
<p>版本库里有暂存区和分支</p>
<p>git add将文件修改添加到暂存区.</p>
<p>git commit将暂存区的所有内容提交到当前分支.</p>
<h1 id="git-checkout-readme-txt"><a href="#git-checkout-readme-txt" class="headerlink" title="git checkout -- readme.txt"></a><code>git checkout -- readme.txt</code></h1><p><code>git checkout</code>其实是用版本库里的版本替换工作区的版本</p>
<p>让readme.txt文件回到上次add或commit之前的状态</p>
<p>只修改了工作区就用<code>git checkout -- readme.txt</code>删除修改,</p>
<p>若添加到了暂存区,就先git reset HEAD <file>再git checkout – file.</file></p>
<h1 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h1><p>git rm file,从版本库中删除</p>
<p>git checkout — file恢复到最新版本</p>
<h1 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h1><p>git remote add <name> <url></url></name></p>
<p><name>默认叫origin</name></p>
<p>添加⼀个新的远程连接。添加后可使⽤<name>作为指定<url>远程连接的名称。</url></name></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure>

<h2 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h2><p>将当前分支推送到远程</p>
<p><code>git push -u origin master</code></p>
<p>-u:不但推送,而且会把当前分支和远程分支关联起来 </p>
<p>之后用<code>git push origin master</code>推送就可以</p>
<h1 id="git-remote-v"><a href="#git-remote-v" class="headerlink" title="git remote -v"></a>git remote -v</h1><p>查看远程库信息</p>
<h1 id="git-remote-rm"><a href="#git-remote-rm" class="headerlink" title="git remote rm "></a>git remote rm <name></name></h1><p>删除远程库</p>
<h2 id="git-clone"><a href="#git-clone" class="headerlink" title="git clone"></a>git clone</h2><p>从远程库克隆,git clone <repo></repo></p>
<p>克隆⼀个指定repo到本地。指定的repo可以是本地⽂件系统或者由HTTP或SSH指定的远程路径。</p>
<h1 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h1><h1 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h1><p>查看分支</p>
<h1 id="git-checkout-b-dev"><a href="#git-checkout-b-dev" class="headerlink" title="git checkout -b dev"></a><code>git checkout -b dev</code></h1><p>创建并切换到当前分支</p>
<p>相当于<code>git branch dev</code>和<code>git checkout dev</code></p>
<h1 id="git-merge"><a href="#git-merge" class="headerlink" title="git merge"></a>git merge</h1><p>合并分支</p>
<p>假如当前分支为master,git merge dev会把dev分支合并到master上。</p>
<h1 id="switch"><a href="#switch" class="headerlink" title="switch"></a>switch</h1><h2 id="git-switch-c-dev"><a href="#git-switch-c-dev" class="headerlink" title="git switch -c dev"></a>git switch -c dev</h2><p>创建并切换到新的dev分支</p>
<h2 id="git-switch-dev"><a href="#git-switch-dev" class="headerlink" title="git switch dev"></a>git switch dev</h2><p>切换到已有的dev分支</p>
<h1 id="关于分支操作"><a href="#关于分支操作" class="headerlink" title="关于分支操作"></a>关于分支操作</h1><p>查看分支：<code>git branch</code></p>
<p>创建分支：<code>git branch &lt;name&gt;</code></p>
<p>切换分支：<code>git checkout &lt;name&gt;</code>或者<code>git switch &lt;name&gt;</code></p>
<p>创建+切换分支：<code>git checkout -b &lt;name&gt;</code>或者<code>git switch -c &lt;name&gt;</code></p>
<p>合并某分支到当前分支：<code>git merge &lt;name&gt;</code></p>
<p>删除分支：<code>git branch -d &lt;name&gt;</code></p>
<h1 id="git-stash"><a href="#git-stash" class="headerlink" title="git stash"></a><code>git stash</code></h1><p>把当前工作现场“储藏”起来，等以后恢复现场后继续工作.当需要切换分支，但还没提交的时候会用到。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git stash</span><br><span class="line"><span class="comment"># 需要恢复时</span></span><br><span class="line">git stash apply恢复，但是恢复后，stash内容并不删除，需要用git stash drop来删除</span><br><span class="line">git stash pop，恢复的同时把stash内容也删除</span><br><span class="line"><span class="comment"># 多次stash</span></span><br><span class="line">git stash list查看所有的stash</span><br><span class="line">git stash apply stash@&#123;0&#125;恢复指定的stash</span><br></pre></td></tr></table></figure>

<h1 id="git-cherry-pick"><a href="#git-cherry-pick" class="headerlink" title="git cherry-pick"></a>git cherry-pick</h1><p>当一个分支需要复制其他分支的修改的时候，<code>git cherry-pick 4c805e2</code>，git会自动做一次提交，但是不同于复制过来的提交，commit_id不同，</p>
<h1 id="git-rebase"><a href="#git-rebase" class="headerlink" title="git rebase"></a>git rebase</h1><p>rebase操作可以把本地未push的分叉提交历史整理成直线。</p>
<p>当冲突时，根据提示手动更改对应冲突文件，然后只需git add ，不需git commit,再git rebase —continue会继续未完成的提交,</p>
<p>无论rebase进行到什么地步,git rebase –abort都会恢复到rebase之前状态.</p>
<h1 id="关于删除中间某次提交的三种方式"><a href="#关于删除中间某次提交的三种方式" class="headerlink" title="关于删除中间某次提交的三种方式"></a>关于删除中间某次提交的三种方式</h1><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0eb17bc695ed">https://www.jianshu.com/p/0eb17bc695ed</a></p>
<h3 id="git-reset"><a href="#git-reset" class="headerlink" title="git reset"></a>git reset</h3><ul>
<li>git reset ：回滚到某次提交。</li>
<li>git reset –soft：此次提交之后的修改会被退回到暂存区</li>
<li>git reset –hard：此次提交之后的修改不做任何保留，git status 查看工作区是没有记录的</li>
</ul>
<p>git rebase：当两个分支不在一条线上，需要执行 merge 操作时使用该命令。<code>进入 Vim 编辑模式，将要删除的 commit 前面的 </code>pick<code>改成</code>drop&#96;&#96;。</p>
<h3 id="git-revert"><a href="#git-revert" class="headerlink" title="git revert"></a>git revert</h3><ul>
<li>git revert：放弃某次提交。<br>git revert 之前的提交仍会保留在 git log 中，而此次撤销会做为一次新的提交。</li>
<li>git revert -m：用于对 merge 节点的操作，-m 指定具体某个提交点。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/27/zookeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/27/zookeeper/" class="post-title-link" itemprop="url">zookeeper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-27 14:10:29 / 修改时间：14:17:59" itemprop="dateCreated datePublished" datetime="2022-04-27T14:10:29+08:00">2022-04-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index"><span itemprop="name">big data</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/zookeeper/" itemprop="url" rel="index"><span itemprop="name">zookeeper</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h1><h1 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h1><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用ls命令来查看当前znode的子节点(可监听)</td>
</tr>
<tr>
<td>-w 监听子节点变化</td>
<td></td>
</tr>
<tr>
<td>-s 附加次级信息</td>
<td></td>
</tr>
<tr>
<td>create</td>
<td>普通创建</td>
</tr>
<tr>
<td>-s 含有序列</td>
<td></td>
</tr>
<tr>
<td>-e 临时(重启或者超时消失)</td>
<td></td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值[可监听]</td>
</tr>
<tr>
<td>-w 监听节点内容变化</td>
<td></td>
</tr>
<tr>
<td>-s 附加次级信息</td>
<td></td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<p>  一个leader，多个follower</p>
<p>半数机制</p>
<p>全局一致性</p>
<p>更新请求顺序执行</p>
<p>数据更新原子性</p>
<p>实时性</p>
<p>服务：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下<br>线、软负载均衡</p>
<h1 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h1><p>第一次启动时：</p>
<p>半数机制</p>
<p>非第一次启动：</p>
<p>已经存在leader：只会同步信息</p>
<p>不存在leader：(EPOCH,ZXID,SID),ZPOCH大的胜出，否则ZXID大的胜出，否则SID大的胜出</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shuaiandjun/p/9383655.html">https://www.cnblogs.com/shuaiandjun/p/9383655.html</a> </p>
<h1 id="节点信息"><a href="#节点信息" class="headerlink" title="节点信息"></a>节点信息</h1><p>重要:</p>
<p>czxid：创建节点的事务 zxid</p>
<p>dataversion：znode数据变化号</p>
<p>dataLength:znode的数据长度 </p>
<p>numChildren:znode子节点数量 </p>
<p>ctime:znode被创建的毫秒数(从1970年开始)</p>
<p>mzxid:znode最后更新的事务zxid </p>
<p>mtime:znode最后修改的毫秒数(从1970年开始) </p>
<p>pZxid:znode最后更新的子节点zxid </p>
<p>cversion:znode子节点变化号,znode子节点修改次数 </p>
<p>aclVersion:znode访问控制列表的变化号 </p>
<p>ephemeralOwner:如果是临时节点，这个是znode拥有者的session id。如果不是临时节点,则为0</p>
<p>节点类型:</p>
<p>第一种分法:</p>
<p>持久(Persistent),短暂(Ephemeral)</p>
<p>第二种分法:</p>
<p>无序号，有序号(Sequential)</p>
<h1 id="监听器"><a href="#监听器" class="headerlink" title="监听器"></a>监听器</h1><p><img src="/2022/04/27/zookeeper/zookeeper%202c8c90d59fd84279afd2c99b8dfca9a3/Untitled.png" alt="Untitled"></p>
<p>只能监听一次，若要监听多次，需要多次注册。</p>
<h2 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">3.5</span><span class="number">.7</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">4.12</span>&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;<span class="built_in">compile</span>&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;curator-framework&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">4.3</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">4.3</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;curator-client&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">4.3</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">// 手动实现</span><br><span class="line">// DistributeLock</span><br><span class="line">package learn.zookeeper.distributelock;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.Log;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.LogFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">List</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line">public <span class="keyword">class</span> <span class="title class_">DistributeLock</span> &#123;</span><br><span class="line">    private String connectString = <span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>;</span><br><span class="line">    private <span class="built_in">int</span> sessionTimeout = <span class="number">90000</span>;</span><br><span class="line">    private ZooKeeper zkClient;</span><br><span class="line">    private CountDownLatch countDownLatch = new CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    private CountDownLatch waitLatch = new CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    private static final Log log = LogFactory.getLog(DistributeLock.<span class="keyword">class</span>);</span><br><span class="line">    private String currentNode;</span><br><span class="line">    private String preNode;</span><br><span class="line"></span><br><span class="line">    public void getConnect() throws IOException, InterruptedException, KeeperException &#123;</span><br><span class="line">        zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123;</span><br><span class="line">            public void process(WatchedEvent event) &#123;</span><br><span class="line">                <span class="keyword">if</span> (event.getState() == Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                    countDownLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (event.getType() == Event.EventType.NodeDeleted &amp;&amp; event.getPath().equals(preNode)) &#123;</span><br><span class="line">                    waitLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        // 等待zk连接到服务器</span><br><span class="line">        <span class="keyword">if</span> (zkClient.getState() == ZooKeeper.States.CONNECTING) &#123;</span><br><span class="line">            countDownLatch.<span class="keyword">await</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 判断根节点locks是否存在，不存在则创建</span><br><span class="line">        <span class="keyword">if</span> (zkClient.exists(<span class="string">&quot;/locks&quot;</span>, false) == null) &#123;</span><br><span class="line">            log.info(<span class="string">&quot;root node not exists, create /locks&quot;</span>);</span><br><span class="line">            zkClient.create(<span class="string">&quot;/locks&quot;</span>, <span class="string">&quot;/locks&quot;</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void zkLock() throws InterruptedException, KeeperException &#123;</span><br><span class="line">        currentNode = zkClient.create(<span class="string">&quot;/locks/&quot;</span> + <span class="string">&quot;seq-&quot;</span>, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line"></span><br><span class="line">        <span class="type">List</span>&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/locks&quot;</span>, false);</span><br><span class="line">        <span class="keyword">if</span> (children.size() == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            Collections.sort(children);</span><br><span class="line">            <span class="built_in">int</span> index = children.indexOf(currentNode.substring(<span class="string">&quot;/locks/&quot;</span>.length()));</span><br><span class="line">            <span class="keyword">if</span> (index == -<span class="number">1</span>) &#123;</span><br><span class="line">                log.info(<span class="string">&quot;数据不存在&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">                    log.info(<span class="string">&quot;获取到锁&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                preNode = <span class="string">&quot;/locks/&quot;</span> + children.get(index - <span class="number">1</span>);</span><br><span class="line">                zkClient.getData(preNode, true, new Stat());</span><br><span class="line">                waitLatch.<span class="keyword">await</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void unZkLock() throws InterruptedException, KeeperException &#123;</span><br><span class="line">        zkClient.delete(currentNode, -<span class="number">1</span>);</span><br><span class="line">        currentNode = null;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 已有框架实现</span><br><span class="line">package learn.zookeeper.distributelock;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.locks.InterProcessMutex;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.ExponentialBackoffRetry;</span><br><span class="line"></span><br><span class="line">public <span class="keyword">class</span> <span class="title class_">CuratorLockTest</span> &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        final InterProcessMutex lock1 = new InterProcessMutex(getCuratorFramework(), <span class="string">&quot;/locks&quot;</span>);</span><br><span class="line">        InterProcessMutex lock2 = new InterProcessMutex(getCuratorFramework(), <span class="string">&quot;/locks&quot;</span>);</span><br><span class="line"></span><br><span class="line">        new Thread(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                lock1.acquire();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1 获取到锁&quot;</span>);</span><br><span class="line"></span><br><span class="line">                lock1.acquire();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1 再次获取到锁&quot;</span>);</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                lock1.release();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1 释放锁&quot;</span>);</span><br><span class="line"></span><br><span class="line">                lock1.release();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1  再次释放锁&quot;</span>);</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        new Thread(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                lock2.acquire();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1 获取到锁&quot;</span>);</span><br><span class="line"></span><br><span class="line">                lock2.acquire();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1 再次获取到锁&quot;</span>);</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                lock2.release();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1 释放锁&quot;</span>);</span><br><span class="line"></span><br><span class="line">                lock2.release();</span><br><span class="line">                System.out.println(<span class="string">&quot;线程1  再次释放锁&quot;</span>);</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static CuratorFramework getCuratorFramework() &#123;</span><br><span class="line">        ExponentialBackoffRetry exponentialBackoffRetry = new ExponentialBackoffRetry(<span class="number">3000</span>, <span class="number">3</span>);</span><br><span class="line">        CuratorFramework curatorFramework = CuratorFrameworkFactory.builder().connectString(<span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>)</span><br><span class="line">                .connectionTimeoutMs(<span class="number">2000</span>)</span><br><span class="line">                .sessionTimeoutMs(<span class="number">2000</span>)</span><br><span class="line">                .retryPolicy(exponentialBackoffRetry)</span><br><span class="line">                .build();</span><br><span class="line">        curatorFramework.start();</span><br><span class="line">        System.out.println(<span class="string">&quot;zookeeper 启动成功&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> curatorFramework;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="zookeeper安装台数"><a href="#zookeeper安装台数" class="headerlink" title="zookeeper安装台数"></a>zookeeper安装台数</h1><p>安装奇数台</p>
<p>10→3</p>
<p>20→5</p>
<p>100→11</p>
<p>200→11</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/27/hadoop-yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/27/hadoop-yarn/" class="post-title-link" itemprop="url">hadoop yarn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-27 14:07:22 / 修改时间：14:17:32" itemprop="dateCreated datePublished" datetime="2022-04-27T14:07:22+08:00">2022-04-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index"><span itemprop="name">big data</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/hadoop/" itemprop="url" rel="index"><span itemprop="name">hadoop</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/hadoop/yarn/" itemprop="url" rel="index"><span itemprop="name">yarn</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h1><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序</p>
<h1 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h1><p><img src="/2022/04/27/hadoop-yarn/yarn%207b6878eb29574145b350acaeb8c2ef1c/Untitled.png" alt="Untitled"></p>
<p>ResourceManager</p>
<p>NodeManager</p>
<p>ApplicationMaster</p>
<p>Container</p>
<h1 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h1><p><img src="/2022/04/27/hadoop-yarn/yarn%207b6878eb29574145b350acaeb8c2ef1c/Untitled%201.png" alt="Untitled"></p>
<ol>
<li>MR程序提交到客户端所在的节点。</li>
<li>YarnRunner向ResourceManager申请一个Application。</li>
<li>RM将该应用程序的资源路径返回给YarnRunner。</li>
<li>该程序将运行所需资源提交到HDFS上。</li>
<li>程序资源提交完毕后，申请运行mrAppMaster。</li>
<li>RM将用户的请求初始化成一个Task。</li>
<li>其中一个NodeManager领取到Task任务。</li>
<li>该NodeManager创建容器Container，并产生MRAppmaster。</li>
<li>Container从HDFS上拷贝资源到本地。</li>
<li>MRAppmaster向RM 申请运行MapTask资源。</li>
<li>RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</li>
<li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</li>
<li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</li>
<li>ReduceTask向MapTask获取相应分区的数据。</li>
<li>程序运行完毕后，MR会向RM申请注销自己。</li>
</ol>
<h1 id="YARN任务提交过程"><a href="#YARN任务提交过程" class="headerlink" title="YARN任务提交过程"></a>YARN任务提交过程</h1><p>（1）作业提交<br>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。<br>第2步：Client向RM申请一个作业id。<br>第3步：RM给Client返回该job资源的提交路径和作业id。<br>第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。<br>第5步：Client提交完资源后，向RM申请运行MrAppMaster。<br>（2）作业初始化<br>第6步：当RM收到Client的请求后，将该job添加到容量调度器中。<br>第7步：某一个空闲的NM领取到该Job。<br>第8步：该NM创建Container，并产生MRAppmaster。<br>第9步：下载Client提交的资源到本地。<br>（3）任务分配<br>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。<br>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。<br>（4）任务运行<br>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>第14步：ReduceTask向MapTask获取相应分区的数据。<br>第15步：程序运行完毕后，MR会向RM申请注销自己。<br>（5）进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。<br>（6）作业完成<br>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<h1 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h1><p>分为:FIFO、Capacity Scheduler和Fair Scheduler.Hadoop2.7.2默认Capacity Scheduler</p>
<p>设置：</p>
<p>yarn-default.xml</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="FIFO"><a href="#FIFO" class="headerlink" title="FIFO"></a>FIFO</h2><p>按到达时间排序，先到先服务</p>
<h2 id="容量调度器"><a href="#容量调度器" class="headerlink" title="容量调度器"></a>容量调度器</h2><p><img src="/2022/04/27/hadoop-yarn/yarn%207b6878eb29574145b350acaeb8c2ef1c/Untitled%202.png" alt="Untitled"></p>
<p>支持多队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略</p>
<p>为防止同一个用户的作业独占队列中的资源，会对同一用户提交的作业所占资源量进行限定</p>
<p>首先，计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列-最闲的</p>
<p>其实，按照作业优先级和提交时间顺序，同时考虑用户资源量限制和内存限制对队列内任务排序</p>
<p>三个队列同时按照任务的先后顺序依次执行，比如，job11,job21和job31分别排在队列最前面，先运行，也是并行运行</p>
<h2 id="公平调度器"><a href="#公平调度器" class="headerlink" title="公平调度器"></a>公平调度器</h2><p><img src="/2022/04/27/hadoop-yarn/yarn%207b6878eb29574145b350acaeb8c2ef1c/Untitled%203.png" alt="Untitled"></p>
<p>支持多队列，每个队列的资源量可以配置，同一队列中的作业公平共享队列中所有资源。</p>
<p>每个队列的job按照优先级分配资源，优先级越高分配的资源越多，但每个job都会分配到资源以确保公平。</p>
<p>资源有限的情况下，每个job理想情况下获得的计算资源与实际获得的计算资源存在一种差距，这个差距就叫做缺额。</p>
<p>同一个队列中，job的资源缺额越大，越先获得资源优先执行。</p>
<h1 id="任务执行推测"><a href="#任务执行推测" class="headerlink" title="任务执行推测"></a>任务执行推测</h1><p>作业完成时间取决于最慢的任务完成时间</p>
<h2 id="推测执行机制"><a href="#推测执行机制" class="headerlink" title="推测执行机制"></a>推测执行机制</h2><p>为拖后腿的任务启动一个备份任务</p>
<h2 id="执行推测任务前提"><a href="#执行推测任务前提" class="headerlink" title="执行推测任务前提"></a>执行推测任务前提</h2><ol>
<li>每个Task只能有一个备份任务</li>
<li>当前Job已完成的Task必须不小于0.05(5%)</li>
<li>开启推测执行参数设置。mapred-site.xml</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol>
<li>不能启用推测任务情况<ol>
<li>任务间存在严重的负载倾斜</li>
<li>特殊任务，比如任务向数据库中写数据</li>
</ol>
</li>
<li>推测执行算法原理<ol>
<li>若某时刻，任务T的执行进度为progress,可通过一定算法推测出该任务的最终完成时刻estimateEndTime.若此刻为该任务启动一个备份任务，可推断出它可能的完成时刻estimateEndTime.<ol>
<li>estimatedRunTime&#x3D;(currentTimestamp-taskStartTime)&#x2F;progress<br> 推测运行时间(60s)&#x3D;(当前时刻(6)-任务启动时刻(0))&#x2F;任务运行比例(10%)</li>
<li>estimateEndTime&#x3D;estimatedRunTime+taskStartTime<br> 推测执行完时刻 60&#x3D;推测运行时间(60s)+任务启动时刻(0) </li>
<li>estimateEndTime&#96;&#x3D;currentTimestamp+averageRunTime<br> 备份任务推测完成时刻(16)&#x3D;当前时刻(6)+运行完成任务的平均时间(10s)</li>
</ol>
</li>
<li>MR总是选择(estimateEndTime-estimateEndTime&#96;)差值最大的任务，并为之启动备份任务</li>
<li>为了防止大量任务同时启动备份任务造成的资源浪费，MR为每个作业设置了同时启动的备份任务数目上限</li>
<li>推测执行机制实际上采用了经典的优化算法：以空间换时间。</li>
</ol>
</li>
</ol>
<h1 id="MapReduce优化"><a href="#MapReduce优化" class="headerlink" title="MapReduce优化"></a>MapReduce优化</h1><h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><p>合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</p>
<p>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。</p>
<h2 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段"></a>Map阶段</h2><p>减少溢写(Splill)次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</p>
<p>减少合并(Merge)次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</p>
<p>在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I&#x2F;O </p>
<h2 id="Reduce阶段"><a href="#Reduce阶段" class="headerlink" title="Reduce阶段"></a>Reduce阶段</h2><p>合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</p>
<p>设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</p>
<p>规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</p>
<p>合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的之后。Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘→读磁盘的过程。既然有这个弊端，那么就可以通过参数来配置，使Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent,默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。</p>
<h2 id="IO传输"><a href="#IO传输" class="headerlink" title="IO传输"></a>IO传输</h2><p>采用数据压缩的方式，减少网络IO的时间。安装Snappy和LZO压缩编码器</p>
<p>使用SequenceFile二进制文件</p>
<h2 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h2><p>现象：</p>
<p>数据频率倾斜—某一个区域的数据量要远远大于其他区域</p>
<p>数据大小倾斜—部分记录的大小远远大于平均值</p>
<p>减少数据倾斜的方法：</p>
<ol>
<li>抽样和范围分区<br>可通过对原始数据进行抽样得到的结果集来预设分区边界值</li>
<li>自定义分区<br>基于输出键的背景知识进行自定义分区。</li>
<li>Combine</li>
<li>采用MapJoin，尽量避免ReduceJoin</li>
</ol>
<h2 id="常用参数调优"><a href="#常用参数调优" class="headerlink" title="常用参数调优"></a>常用参数调优</h2><p>可在MR程序中配置</p>
<p>mapred-default.xml</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数。默认值是5</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例。默认值0.7</td>
</tr>
<tr>
<td>mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td>
</tr>
</tbody></table>
<p>需在YARN启动之前配置</p>
<p>yarn-default.xml</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192</td>
</tr>
</tbody></table>
<p>SHUFFLE性能优化的参数，需YARN启动之前配置</p>
<p>mapred-default.xml</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认100m</td>
</tr>
<tr>
<td>mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认80%</td>
</tr>
</tbody></table>
<p>容错相关参数：</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td>
</tr>
</tbody></table>
<h1 id="小文件"><a href="#小文件" class="headerlink" title="小文件"></a>小文件</h1><p>占用NameNode内存，索引文件过大，索引变慢</p>
<p>解决：</p>
<p>Hadoop archive</p>
<p>sequence file</p>
<p>combinefileinputformat</p>
<p>jvm重用</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/27/hadoop-mapreduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/27/hadoop-mapreduce/" class="post-title-link" itemprop="url">hadoop mapreduce</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-27 14:03:21 / 修改时间：14:17:18" itemprop="dateCreated datePublished" datetime="2022-04-27T14:03:21+08:00">2022-04-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index"><span itemprop="name">big data</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/hadoop/" itemprop="url" rel="index"><span itemprop="name">hadoop</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/hadoop/mapreduce/" itemprop="url" rel="index"><span itemprop="name">mapreduce</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p>是一个分布式运算程序的编程框架</p>
<p>优点：</p>
<ol>
<li>易于编程<br>简单的实现一些接口，就可以完成一个分布式程序</li>
<li>良好的扩展性</li>
<li>高容错性<br>一台机器挂了，可以把计算任务转移到另一台节点上运行</li>
<li>适合PB级以上海量数据的离线处理</li>
</ol>
<p>缺点：</p>
<ol>
<li>不擅长实时计算</li>
<li>不擅长流式计算</li>
<li>不擅长DAG(有向图)计算<br>多个MapReduce作业，每个输出结果都会写到磁盘，造成大量磁盘IO，导致性能低下</li>
</ol>
<h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h1><p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled.png" alt="Untitled"></p>
<p>有两个阶段：</p>
<p>MapTask并发实例，完全并行运行，互不相干</p>
<p>ReduceTask并发实例，互不相干，但他们的数据依赖于上一个阶段的所有MapTask并发实例的输出</p>
<p>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段</p>
<h1 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h1><p>MrAppMaster:负责整个程序的过程调度及状态协调</p>
<p>MapTask:负责Map阶段的整个数据处理流程 </p>
<p>ReduceTask:负责Reduce阶段的整个数据处理流程 </p>
<h1 id="常用Hadoop数据序列化类型"><a href="#常用Hadoop数据序列化类型" class="headerlink" title="常用Hadoop数据序列化类型"></a>常用Hadoop数据序列化类型</h1><table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td>boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
<h1 id="编程规范"><a href="#编程规范" class="headerlink" title="编程规范"></a>编程规范</h1><p>分为:Mapper、Reducer、Driver </p>
<p>Mapper阶段:</p>
<ol>
<li>用户自定义的Mapper要继承自己的父类</li>
<li>Mapper的输入数据是KV对形式</li>
<li>Mapper中的业务逻辑写在map()方法中</li>
<li>输出数据是KV对形式</li>
<li>map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次</li>
</ol>
<p>Reducer阶段:</p>
<ol>
<li>用户自定义的Reducer要继承自己的父类</li>
<li>Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</li>
<li>Reducer的业务逻辑写在reduce()方法中</li>
<li>ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</li>
</ol>
<p>Driver阶段：</p>
<p>相当于YARN集群的客户端，用于提交整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象</p>
<h1 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;log4j-core&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.8.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.7.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.7.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.7.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<h1 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h1><p>Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordcountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">	<span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">	<span class="type">IntWritable</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		<span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">		<span class="comment">// 2 切割</span></span><br><span class="line">		String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">		<span class="comment">// 3 输出</span></span><br><span class="line">		<span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">			k.set(word);</span><br><span class="line">			context.write(k, v);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.wordcount;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordcountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line">	<span class="type">int</span> sum;</span><br><span class="line">	<span class="type">IntWritable</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;	</span><br><span class="line">		<span class="comment">// 1 累加求和</span></span><br><span class="line">		sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">			sum += count.get();</span><br><span class="line">		&#125;	</span><br><span class="line">		<span class="comment">// 2 输出</span></span><br><span class="line">    v.set(sum);</span><br><span class="line">		context.write(key,v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.wordcount;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordcountDriver</span> &#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">		<span class="comment">// 1 获取配置信息以及封装任务</span></span><br><span class="line">		<span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">		<span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line">		<span class="comment">// 2 设置jar加载路径</span></span><br><span class="line">		job.setJarByClass(WordcountDriver.class);</span><br><span class="line">		<span class="comment">// 3 设置map和reduce类</span></span><br><span class="line">		job.setMapperClass(WordcountMapper.class);</span><br><span class="line">		job.setReducerClass(WordcountReducer.class);</span><br><span class="line">		<span class="comment">// 4 设置map输出</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">		<span class="comment">// 5 设置最终输出kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line">		<span class="comment">// 6 设置输入和输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">		<span class="comment">// 7 提交</span></span><br><span class="line">		<span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h1><p>把内存中的对象转换成字节序列(或其他数据传输协议)以便存储到磁盘(持久化)和网络传输</p>
<p>反序列化就是将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象</p>
<p>序列化的作用：可以存储”活的”对象，将”活的”对象发送到远程计算机.</p>
<p>Java序列化是一个重量级序列化框架(Serializable),一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等),不便于在网络中高效传输。所以，Hadoop开发了一套序列化机制(Writable).</p>
<p>Hadoop序列化特点：紧凑、快速、可扩展、互操作</p>
<h2 id="自定义bean对象实现序列化接口-Writable"><a href="#自定义bean对象实现序列化接口-Writable" class="headerlink" title="自定义bean对象实现序列化接口(Writable)"></a>自定义bean对象实现序列化接口(Writable)</h2><p>七个步骤：</p>
<ol>
<li>必须实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li>
<li>重写序列化方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">	out.writeLong(upFlow);</span><br><span class="line">	out.writeLong(downFlow);</span><br><span class="line">	out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>重写反序列化方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">	upFlow = in.readLong();</span><br><span class="line">	downFlow = in.readLong();</span><br><span class="line">	sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>注意反序列化的顺序和序列化的顺序完全一致</li>
<li>要把结果显示在文件中，要重写toString(),可用”\t”分开</li>
<li>若要将自定义bean放在key中传输，需实现Comparable接口，因为shuffle过程要求对key必须能排序</li>
</ol>
<h1 id="InputFormat输入"><a href="#InputFormat输入" class="headerlink" title="InputFormat输入"></a>InputFormat输入</h1><h2 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h2><h3 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h3><p>数据块：Block是HDFS物理上把数据分成一块一块</p>
<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储</p>
<h2 id="split切片"><a href="#split切片" class="headerlink" title="split切片"></a>split切片</h2><p>一个Job的Map阶段并行度由客户端在提交Job时的切片数决定</p>
<p>每一个Split切片分配一个MapTask并行实例处理</p>
<p>默认情况下，切片大小&#x3D;BlockSize</p>
<p>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</p>
<p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%201.png" alt="Untitled"></p>
<h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><ol>
<li>程序先找到数据存储的目录</li>
<li>开始遍历处理(规划切片)目录下的每一个文件</li>
<li>遍历第一个文件<ol>
<li>获取文件大小fs.sizeOf(ss.txt)</li>
<li>计算切片大小<ol>
<li>computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M</li>
</ol>
</li>
<li>默认情况下，切片大小&#x3D;blocksize</li>
<li>开始切，形成第一个，第二个，，，切片(每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片)</li>
<li>将切片信息写道一个切片规划文件中</li>
<li>整个切片的核心过程在getSplit()方法中完成</li>
<li>InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在节点列表等</li>
</ol>
</li>
<li>提交切片规划文件到YARN上，ＹＡＲＮ上的ＭｒＡｐｐＭａｓｔｅｒ就可以根据切片规划文件计算开启ＭａｐＴａｓｋ个数</li>
</ol>
<h2 id="计算切片大小的公式"><a href="#计算切片大小的公式" class="headerlink" title="计算切片大小的公式"></a>计算切片大小的公式</h2><p>Math.max(minSize,Math.min(maxSize,blockSize))</p>
<p>mapreduce.input.fileinputformat.split.minsize&#x3D;1,默认值为1</p>
<p>mapreduce.input.fileinputformat.split.maxsize&#x3D;Long.MAXValue默认值为Long.MAXValue</p>
<p>所以，默认情况下，切片大小&#x3D;blocksize.</p>
<h3 id="切片信息API"><a href="#切片信息API" class="headerlink" title="切片信息API"></a>切片信息API</h3><p>&#x2F;&#x2F; 获取切片的文件名称</p>
<p>String name &#x3D; inputSplit.getPath().getName();</p>
<p>&#x2F;&#x2F; 根据文件类型获取切片信息</p>
<p>FileSplit inputSplit &#x3D; (FileSplit) context.getInputSplit();</p>
<h2 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a>CombineTextInputFormat切片机制</h2><p>用于小文件过多的场景</p>
<h3 id="虚拟存储切片最大值设置"><a href="#虚拟存储切片最大值设置" class="headerlink" title="虚拟存储切片最大值设置"></a>虚拟存储切片最大值设置</h3><p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);&#x2F;&#x2F; 4m</p>
<p>生成切片过程包括：虚拟存储过程和切片过程</p>
<ol>
<li>虚拟存储过程<br>将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）</li>
<li>切片过程<ol>
<li>判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片</li>
<li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片</li>
</ol>
</li>
</ol>
<p>Driver类设置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure>

<h2 id="FileInput实现类"><a href="#FileInput实现类" class="headerlink" title="FileInput实现类"></a>FileInput实现类</h2><p>TextInputFormat、KeyValueInputFormat、LNlineInputFormat、CombineTextInputFormat和自定义InputFormat等</p>
<h3 id="TextInputFormat"><a href="#TextInputFormat" class="headerlink" title="TextInputFormat"></a>TextInputFormat</h3><p>默认实现类，按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，LongWritable类型。值是这行的内容，不包括任何行终止符(换行符和回车符),Text类型</p>
<h3 id="KeyValueInputFormat"><a href="#KeyValueInputFormat" class="headerlink" title="KeyValueInputFormat"></a>KeyValueInputFormat</h3><p>每一行均为一条记录，被分割符分割为key,value。可以通过在驱动类中设置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line">conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR,”\t”);</span><br></pre></td></tr></table></figure>

<p>来设定分隔符，默认tab(\t).</p>
<h3 id="NLineInputFormat"><a href="#NLineInputFormat" class="headerlink" title="NLineInputFormat"></a>NLineInputFormat</h3><p>不再按Block块去划分，按NLineInputFormat指定的行数N来划分，即输入文件总行数&#x2F;N&#x3D;切片数，若不整除，切片数&#x3D;商+1</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 7设置每个切片InputSplit中划分三条记录</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line"><span class="comment">// 8使用NLineInputFormat处理记录数  </span></span><br><span class="line">job.setInputFormatClass(NLineInputFormat.class);</span><br></pre></td></tr></table></figure>

<h2 id="自定义InputFormat"><a href="#自定义InputFormat" class="headerlink" title="自定义InputFormat"></a>自定义InputFormat</h2><p>案例：</p>
<p>将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value</p>
<p>流程：</p>
<ol>
<li>自定义一个类继承FileInputFormat<ol>
<li>重写isSplitable()方法,返回false不可切割</li>
<li>重写createRecordReader(),创建自定义的RecordReader对象，并初始化</li>
</ol>
</li>
<li>改写RecordReader，实现一次读取一个完整文件封装到KV<ol>
<li>采用IO流读取一个文件输出到value中，设置不可切片，最终把所有文件都封装到value中</li>
<li>获取文件路径信息+名称，并设置key</li>
</ol>
</li>
<li>设置Driver<ol>
<li>&#x2F;&#x2F;设置输入的inputFormat<br> job.setInputFormatClass(WholeFileInputFormat.class);</li>
<li>&#x2F;&#x2F; 设置输出的outputForma<br> job.setOutputFormat(SequenceFileOutputFormat.class);</li>
</ol>
</li>
</ol>
<p>代码：</p>
<p>自定义InputFormat</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.inputformat;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="comment">// 定义类继承FileInputFormat</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WholeFileInputformat</span> <span class="keyword">extends</span> <span class="title class_">FileInputFormat</span>&lt;Text, BytesWritable&gt;&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="type">boolean</span> <span class="title function_">isSplitable</span><span class="params">(JobContext context, Path filename)</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title function_">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span>	<span class="keyword">throws</span> IOException, InterruptedException &#123;		</span><br><span class="line">		<span class="type">WholeRecordReader</span> <span class="variable">recordReader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">WholeRecordReader</span>();</span><br><span class="line">		recordReader.initialize(split, context);	</span><br><span class="line">		<span class="keyword">return</span> recordReader;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义RecordReader：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.inputformat;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title class_">RecordReader</span>&lt;Text, BytesWritable&gt;&#123;</span><br><span class="line">	<span class="keyword">private</span> Configuration configuration;</span><br><span class="line">	<span class="keyword">private</span> FileSplit split;	</span><br><span class="line">	<span class="keyword">private</span> <span class="type">boolean</span> isProgress= <span class="literal">true</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="type">BytesWritable</span> <span class="variable">value</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BytesWritable</span>();</span><br><span class="line">	<span class="keyword">private</span> <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;		</span><br><span class="line">		<span class="built_in">this</span>.split = (FileSplit)split;</span><br><span class="line">		configuration = context.getConfiguration();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;		</span><br><span class="line">		<span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line">			<span class="comment">// 1 定义缓存区</span></span><br><span class="line">			<span class="type">byte</span>[] contents = <span class="keyword">new</span> <span class="title class_">byte</span>[(<span class="type">int</span>)split.getLength()];			</span><br><span class="line">			<span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">			<span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="literal">null</span>;		</span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="comment">// 2 获取文件系统</span></span><br><span class="line">				<span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> split.getPath();</span><br><span class="line">				fs = path.getFileSystem(configuration);				</span><br><span class="line">				<span class="comment">// 3 读取数据</span></span><br><span class="line">				fis = fs.open(path);				</span><br><span class="line">				<span class="comment">// 4 读取文件内容</span></span><br><span class="line">				IOUtils.readFully(fis, contents, <span class="number">0</span>, contents.length);				</span><br><span class="line">				<span class="comment">// 5 输出文件内容</span></span><br><span class="line">				value.set(contents, <span class="number">0</span>, contents.length);</span><br><span class="line">				<span class="comment">// 6 获取文件路径及名称</span></span><br><span class="line">				<span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> split.getPath().toString();				</span><br><span class="line">				<span class="comment">// 7 设置输出的key值</span></span><br><span class="line">				k.set(name);</span><br><span class="line">			&#125; <span class="keyword">catch</span> (Exception e) &#123;			</span><br><span class="line">			&#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">				IOUtils.closeStream(fis);</span><br><span class="line">			&#125;			</span><br><span class="line">			isProgress = <span class="literal">false</span>;			</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">		&#125;		</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> Text <span class="title function_">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">		<span class="keyword">return</span> k;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> BytesWritable <span class="title function_">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">		<span class="keyword">return</span> value;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="type">float</span> <span class="title function_">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h1><p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%202.png" alt="Untitled"></p>
<p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%203.png" alt="Untitled"></p>
<p>SHUFFLE过程为第7-12步</p>
<ol>
<li>MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</li>
<li>多个溢出文件会被合并成大的溢出文件</li>
<li>在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li>
<li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</li>
<li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</li>
</ol>
<p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。<br>缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M</p>
<p>源码解析流程：</p>
<p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%204.png" alt="Untitled"></p>
<h1 id="SHUFLLE机制"><a href="#SHUFLLE机制" class="headerlink" title="SHUFLLE机制"></a>SHUFLLE机制</h1><p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%205.png" alt="Untitled"></p>
<h1 id="Partition分区"><a href="#Partition分区" class="headerlink" title="Partition分区"></a>Partition分区</h1><p>默认分区:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>默认分区根据key的hashCode对ReduceTasks个数取模得到。用户没法控制哪个key存储到哪个分区</p>
<h2 id="自定义Partitioner步骤"><a href="#自定义Partitioner步骤" class="headerlink" title="自定义Partitioner步骤"></a>自定义Partitioner步骤</h2><ol>
<li>自定义类继承Partitioner,重写getPartition()方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartition</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key, FlowBean value, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">		<span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">		...</span><br><span class="line">			<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>在job驱动中，设置自定义Partitioner<br>job.setPartitionerClass(CustomPartitioner.class);</li>
<li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask<br>job.setNumReduceTasks(5);</li>
</ol>
<p>如果ReduceTask的数量&gt;getPartition的结果数，则会多产生几个空的输出文件</p>
<p>如果1&lt;ReduceTask数量&lt;getPartition结果数，则有一部分分区数据无处安放，会Exception</p>
<p>若ReduceTask数量&#x3D;1，只会产生一个结果文件</p>
<p>分区号必须从零开始，逐一累加</p>
<h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><p>MapTask和ReduceTask中，任何应用程序的数据均会被排序，无论逻辑上是否需要。</p>
<p>默认字典排序，快排</p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>部分排序</p>
<ol>
<li>MapReduce根据输入记录的键对数据集排序，保证输出的每个文件内部有序</li>
<li>全排序<br>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask.但效率极低</li>
<li>辅助排序:GroupingComparator分组<br>在Reduce端对key进行分组。应用于：key为bean对象时，想让key进入同一reduce方法.(只有partion分区分在同一个分区才会进入同一个Reduce端)</li>
<li>二次排序<br>自定义排序过程中，若compareTo中判断条件为两个即为二次排序</li>
</ol>
<h1 id="Combiner合并"><a href="#Combiner合并" class="headerlink" title="Combiner合并"></a>Combiner合并</h1><p>是Mapper和Reducer之外的一种组件</p>
<p>父类就是Reducer.</p>
<p>Combiner和Reducer之间的区别:</p>
<p>Combiner在每一个MapTask所在的节点运行</p>
<p>Reducer是接收全局所有的Mapper的输出结果</p>
<p>意义：对每个Mapper输出汇总，减少网络传输</p>
<p>前提：不影响业务逻辑，且和Reuducerkv要对应</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordcountCombiner.class);</span><br></pre></td></tr></table></figure>

<h1 id="GroupingComparator分组"><a href="#GroupingComparator分组" class="headerlink" title="GroupingComparator分组"></a>GroupingComparator分组</h1><p>map端数据根据分区进入相应Reducer端，GroupingComparator对Reduce阶段所有的数据根据某一个或几个字段进行分组</p>
<p>步骤：</p>
<ol>
<li>自定义类继承WritableComparator</li>
<li>重写compare()方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(WritableComparable a,WritableComparable b)</span> &#123;</span><br><span class="line">	<span class="comment">// 比较业务逻辑</span></span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>创建一个构造将比较对象的类传给父类</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="title function_">OrderGroupingComparator</span><span class="params">()</span> &#123;</span><br><span class="line">	<span class="built_in">super</span>(OrderBean.class, <span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h1><p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%206.png" alt="Untitled"></p>
<p>（1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key&#x2F;value。<br>（2）Map阶段：该节点主要是将解析出的key&#x2F;value交给用户编写map()函数处理，并产生一系列新的key&#x2F;value。<br>（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key&#x2F;value分区（调用Partitioner），并写入一个环形内存缓冲区中。<br>（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。<br>溢写阶段详情：<br>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。<br>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output&#x2F;spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。<br>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output&#x2F;spillN.out.index中。<br>（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。<br>当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output&#x2F;file.out中，同时生成相应的索引文件output&#x2F;file.out.index。<br>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。<br>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h1 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h1><p><img src="/2022/04/27/hadoop-mapreduce/MapReduce%20077b6275029b42eb9ab8a3b8e8e66c1a/Untitled%207.png" alt="Untitled"></p>
<p>（1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。<br>（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。<br>（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。<br>（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
<h2 id="设置ReduceTask并行度（个数）"><a href="#设置ReduceTask并行度（个数）" class="headerlink" title="设置ReduceTask并行度（个数）"></a>设置ReduceTask并行度（个数）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<p>ReduceTask&#x3D;0,表示没有Reduce阶段，输出文件个数和Map个数一致</p>
<p>ReduceTask默认值就是1，所以输出文件个数为1个</p>
<p>如果数据分部不均匀，就有可能在Reduce阶段产生数据倾斜</p>
<p>若分区数不是1，但ReduceTask为1，则不执行分区过程。MapTask源码中会判断</p>
<h1 id="join"><a href="#join" class="headerlink" title="join"></a>join</h1><h2 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 6 加载缓存数据</span></span><br><span class="line">		job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///e:/input/inputcache/pd.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 7 Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span></span><br><span class="line">		job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取缓存的文件</span></span><br><span class="line">		URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">		<span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> cacheFiles[<span class="number">0</span>].getPath().toString();</span><br><span class="line">		</span><br><span class="line">		<span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(path), <span class="string">&quot;UTF-8&quot;</span>))</span><br></pre></td></tr></table></figure>

<h1 id="计数器的应用"><a href="#计数器的应用" class="headerlink" title="计数器的应用"></a>计数器的应用</h1><p>API:</p>
<ol>
<li>采用枚举的方式统计计数<br>enum MyCounter{MALFORORMED,NORMAL}<br>&#x2F;&#x2F;对枚举定义的自定义计数器加1<br>context.getCounter(MyCounter.MALFORORMED).increment(1)</li>
<li>采用计数器组、计数器名称的方式统计<br>context.getCounter(”counterGroup”, “counter”).increment(1)<br>组名和计数器名称随便起，但最好有意义</li>
<li>计数结果在程序运行后的控制台上查看</li>
</ol>
<h1 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h1><p>压缩基本原则:</p>
<p>运算密集型的job，少用压缩</p>
<p>IO密集型的job，多用压缩</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否，需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码&#x2F;解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能:</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB&#x2F;s</td>
<td>58MB&#x2F;s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB&#x2F;s</td>
<td>9.5MB&#x2F;s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB&#x2F;s</td>
<td>74.6MB&#x2F;s</td>
</tr>
</tbody></table>
<p>Gzip压缩:当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式</p>
<p>Bzip2压缩：适合对速度要求不高，但需要较高的压缩率的时候，或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用的比较少的情况；或对单个很大的文本文件想压缩减少存储空间，同事又需要支持Split，而且兼容之前的应用程序的情况。</p>
<p>Lzo压缩：Hadoop中最流行的压缩格式，一个很大的文本文件，压缩后还大于200M以上的可以考虑，而且单个文件越大，Lzo有点越明显。</p>
<p>Snappy压缩：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</p>
<h2 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h2><p>可以在MapReduce任意阶段启用</p>
<h3 id="输入端"><a href="#输入端" class="headerlink" title="输入端"></a>输入端</h3><p>无需显示指定使用的编解码方式，Hadoop自动检测文件扩展名，如果扩展名能够匹配，就会用恰当的编解码方式对文件进行压缩和解压。否则，不会使用任何编解码器。</p>
<h3 id="Mapper输出端"><a href="#Mapper输出端" class="headerlink" title="Mapper输出端"></a>Mapper输出端</h3><p>若数据量大造成网络传输缓慢，应该考虑使用压缩技术</p>
<h3 id="Reducer输出端"><a href="#Reducer输出端" class="headerlink" title="Reducer输出端"></a>Reducer输出端</h3><p>压缩能减少要存储的数据量，因此降低所需的磁盘空间。</p>
<h2 id="参数压缩配置"><a href="#参数压缩配置" class="headerlink" title="参数压缩配置"></a>参数压缩配置</h2><table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs   （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h2 id="压缩与解压缩代码"><a href="#压缩与解压缩代码" class="headerlink" title="压缩与解压缩代码"></a>压缩与解压缩代码</h2><p>createOutputStream(OutputStreamout)&#x2F;createInputStream(InputStreamin)创建CompressionOutputStream&#x2F;CompressionInputStream</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.compress;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileNotFoundException;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodecFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestCompress</span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		compress(<span class="string">&quot;e:/hello.txt&quot;</span>,<span class="string">&quot;org.apache.hadoop.io.compress.BZip2Codec&quot;</span>);</span><br><span class="line"><span class="comment">//		decompress(&quot;e:/hello.txt.bz2&quot;);</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1、压缩</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">compress</span><span class="params">(String filename, String method)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （1）获取输入流</span></span><br><span class="line">		<span class="type">FileInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename));</span><br><span class="line">		</span><br><span class="line">		<span class="type">Class</span> <span class="variable">codecClass</span> <span class="operator">=</span> Class.forName(method);</span><br><span class="line">		</span><br><span class="line">		<span class="type">CompressionCodec</span> <span class="variable">codec</span> <span class="operator">=</span> (CompressionCodec) ReflectionUtils.newInstance(codecClass, <span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （2）获取输出流</span></span><br><span class="line">		<span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename + codec.getDefaultExtension()));</span><br><span class="line">		<span class="type">CompressionOutputStream</span> <span class="variable">cos</span> <span class="operator">=</span> codec.createOutputStream(fos);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （3）流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(fis, cos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="literal">false</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （4）关闭资源</span></span><br><span class="line">		cos.close();</span><br><span class="line">		fos.close();</span><br><span class="line">		fis.close();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2、解压缩</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">decompress</span><span class="params">(String filename)</span> <span class="keyword">throws</span> FileNotFoundException, IOException &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （0）校验是否能解压缩</span></span><br><span class="line">		<span class="type">CompressionCodecFactory</span> <span class="variable">factory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CompressionCodecFactory</span>(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">		<span class="type">CompressionCodec</span> <span class="variable">codec</span> <span class="operator">=</span> factory.getCodec(<span class="keyword">new</span> <span class="title class_">Path</span>(filename));</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (codec == <span class="literal">null</span>) &#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;cannot find codec for file &quot;</span> + filename);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （1）获取输入流</span></span><br><span class="line">		<span class="type">CompressionInputStream</span> <span class="variable">cis</span> <span class="operator">=</span> codec.createInputStream(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename)));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （2）获取输出流</span></span><br><span class="line">		<span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(filename + <span class="string">&quot;.decoded&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （3）流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(cis, fos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="literal">false</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （4）关闭资源</span></span><br><span class="line">		cis.close();</span><br><span class="line">		fos.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line">		</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/27/hadoop-hdfs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="songjj">
      <meta itemprop="description" content="编程学习之旅">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据学习笔记">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/27/hadoop-hdfs/" class="post-title-link" itemprop="url">hadoop hdfs</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-27 11:57:11 / 修改时间：14:17:06" itemprop="dateCreated datePublished" datetime="2022-04-27T11:57:11+08:00">2022-04-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index"><span itemprop="name">big data</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/hadoop/" itemprop="url" rel="index"><span itemprop="name">hadoop</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/hadoop/hdfs/" itemprop="url" rel="index"><span itemprop="name">hdfs</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h1><p>大数据特点：Volume大量、Velocity高速、Variety多样、Value低价值密度</p>
<p>部门组织结构：</p>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled.png" alt="Untitled"></p>
<p>4高：高可靠性、高扩展性、高效性、高容错性</p>
<p>1.x和2.x的区别：</p>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%201.png" alt="Untitled"></p>
<p>大数据技术生态体系：</p>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%202.png" alt="Untitled"></p>
<h1 id="推荐系统架构"><a href="#推荐系统架构" class="headerlink" title="推荐系统架构"></a>推荐系统架构</h1><p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%203.png" alt="Untitled"></p>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><p>hadoop-env.sh</p>
<p>export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</p>
<p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="http://yarn-env.sh/">yarn-env.sh</a> </p>
<p>export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</p>
<p>yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="http://mapred-env.sh/">mapred-env.sh</a> </p>
<p>export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</p>
<p>mapred-site.xml.template→mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在YARN上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>etc&#x2F;hadoop&#x2F;slaves(不能有空格，空行)</p>
<p>hadoop102</p>
<p>hadoop103</p>
<p>hadoop104</p>
<h2 id="启动-x2F-关闭命令"><a href="#启动-x2F-关闭命令" class="headerlink" title="启动&#x2F;关闭命令"></a>启动&#x2F;关闭命令</h2><p>集群启动&#x2F;停止： </p>
<p>hdfs：<a target="_blank" rel="noopener" href="http://hadoop-daemon.sh/">hadoop-daemon.sh</a> start &#x2F; stop namenode &#x2F; datanode &#x2F; secondarynamenode </p>
<p>yarn：<a target="_blank" rel="noopener" href="http://yarn-daemon.sh/">yarn-daemon.sh</a> start &#x2F; stop resourcemanager &#x2F; nodemanager </p>
<p>集群整体启动&#x2F;停止hdfs:<a target="_blank" rel="noopener" href="http://start-dfs.sh/">start-dfs.sh</a> &#x2F; <a target="_blank" rel="noopener" href="http://stop-dfs.sh/">stop-dfs.sh</a> </p>
<p>Yarn:<a target="_blank" rel="noopener" href="http://start-yarn.sh/">start-yarn.sh</a> &#x2F; <a target="_blank" rel="noopener" href="http://stop-yarn.sh/">stop-yarn.sh</a> </p>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>第一次启动时格式化命令：bin&#x2F;hdfs namenode -format </p>
<p>启动namenode:sbin&#x2F;hadoop-daemon.sh start namenode </p>
<p>启动datanode:sbin&#x2F;hadoop-daemon.sh start datanode </p>
<p>格式化NameNode，会产生新的集群id,导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode</p>
<p>集群第一次启动格式化,然后</p>
<p>sbin&#x2F;start-dfs.sh</p>
<p>启动yarn(在ResourceManager所在节点上启动):sbin&#x2F;start-yarn.sh </p>
<h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>启动ResourceManager:sbin&#x2F;yarn-daemon.sh start resourcemanager </p>
<p>sbin&#x2F;yarn-daemon.sh stop resourcemanager</p>
<p>启动NodeManager:sbin&#x2F;yarn-daemon.sh start nodemanager</p>
<p>sbin&#x2F;yarn-daemon.sh stop nodemanager</p>
<h2 id="历史服务器"><a href="#历史服务器" class="headerlink" title="历史服务器"></a>历史服务器</h2><p>启动历史服务器：sbin&#x2F;mr-jobhistory-daemon.sh start historyserver</p>
<h2 id="web端"><a href="#web端" class="headerlink" title="web端"></a>web端</h2><p>查看HDFS系统：hadoop101:50070 </p>
<p>查看YARN：<a target="_blank" rel="noopener" href="http://hadoop101:8088/cluster">http://hadoop101:8088/cluster</a> </p>
<p>查看日志：<a target="_blank" rel="noopener" href="http://hadoop101:19888/jobhistory">http://hadoop101:19888/jobhistory</a> </p>
<p>查看SecondaryNameNode:<a target="_blank" rel="noopener" href="http://hadoop104:50090/status.html">http://hadoop104:50090/status.html</a></p>
<h2 id="Log日志"><a href="#Log日志" class="headerlink" title="Log日志"></a>Log日志</h2><p>&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;logs </p>
<h2 id="RSYNC"><a href="#RSYNC" class="headerlink" title="RSYNC"></a>RSYNC</h2><p>远程同步命令</p>
<p>用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点，rsync只对差异文件做更新</p>
<p>rsync -rvl $pdir&#x2F;$fname $user@hadoop$host:$pdir&#x2F;$fname</p>
<p><a target="_blank" rel="noopener" href="https://www.notion.so/e26dd4bdee99421c9c12132880b2c9e7">Untitled</a></p>
<p>在~&#x2F;bin下存放脚本呢，该用户可在系统任何地方执行</p>
<h2 id="集群部署规划"><a href="#集群部署规划" class="headerlink" title="集群部署规划"></a>集群部署规划</h2><p><a target="_blank" rel="noopener" href="https://www.notion.so/8b460d832117483bbb212e53d922472c">Untitled</a></p>
<h2 id="时间同步"><a href="#时间同步" class="headerlink" title="时间同步"></a>时间同步</h2><p>ntp</p>
<h1 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h1><p>Hadoop Distributed File System,分布式文件管理系统</p>
<p>高容错性、适合处理大数据、可构建在廉价机器上、不适合低延时数据访问、无法高效的对大量小文件进行存储、进支持追加</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>NameNode(nn):就是Master，它是一个主管、管理者<ul>
<li>管理HDFS的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块(Block)映射信息</li>
<li>处理客户端读写请求</li>
</ul>
</li>
<li>DataNode:就是Slave,NameNode下达命令，DataNode执行实际的操作<ul>
<li>存储实际的数据块</li>
<li>执行数据块的读&#x2F;写操作</li>
</ul>
</li>
<li>Client:就是客户端<ul>
<li>文件切分，文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据</li>
<li>提供一些命令来管理HDFS，如NameNode格式化</li>
<li>可以通过一些命令来访问HDFS,比如对HDFS增删改查操作</li>
</ul>
</li>
<li>Secondary NameNode:非NameNode的热备。当NameNode挂掉时，并不能马上替换NameNode提供服务<ul>
<li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits,并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode</li>
</ul>
</li>
</ul>
<h2 id="HDFS文件块大小"><a href="#HDFS文件块大小" class="headerlink" title="HDFS文件块大小"></a>HDFS文件块大小</h2><p>参数:dfs.blocksize </p>
<p>默认:</p>
<p>hadoop2.x→128M</p>
<p>以前→64M</p>
<p>定为128M原因:</p>
<p>寻址时间为10ms→寻址时间为传输时间的1%为最佳，所以传输时间为:10ms&#x2F;1%&#x3D;1s→磁盘传输速率约为100M&#x2F;s→block块大小:1sX100M&#x2F;s&#x3D;100M </p>
<p>故HDFS块的大小设置主要取决于磁盘传输速率</p>
<h2 id="Shell操作"><a href="#Shell操作" class="headerlink" title="Shell操作"></a>Shell操作</h2><p>bin&#x2F;hadoop fs or bin&#x2F;hdfs dfs </p>
<p>dfs为fs的实现类</p>
<h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-checksum &lt;src&gt; ...]</span><br><span class="line">[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-count [-q] &lt;path&gt; ...]</span><br><span class="line">[-<span class="built_in">cp</span> [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">[-expunge]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-touchz &lt;path&gt; ...]</span><br><span class="line">[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">-<span class="built_in">help</span></span><br><span class="line">hadoop fs -<span class="built_in">help</span> <span class="built_in">rm</span> </span><br><span class="line">-<span class="built_in">ls</span>:显示目录信息</span><br><span class="line">hadoop fs -<span class="built_in">ls</span> /</span><br><span class="line">-<span class="built_in">mkdir</span>:创建目录</span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> -p /sanguo/shuguo </span><br><span class="line">-moveFromLocal:从本地剪切粘贴到HDFS</span><br><span class="line">hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br><span class="line">-appendToFile:追加一个文件到已经存在的文件末尾</span><br><span class="line">hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br><span class="line">-<span class="built_in">cat</span>:显示文件内容</span><br><span class="line">hadoop fs -<span class="built_in">cat</span> /sanguo/shuguo/kongming.txt</span><br><span class="line">-<span class="built_in">chgrp</span>、-<span class="built_in">chmod</span>、-<span class="built_in">chown</span>:修改文件所属权限</span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 666 /sanguo/shuguo/kongming.txt</span><br><span class="line">hadoop fs -<span class="built_in">chown</span> atguigu:atguigu /sanguo/shuguo/kongming.txt</span><br><span class="line">-copyFromLocal:从本地文件系统中拷贝文件到HDFS路径去</span><br><span class="line">hadoop fs -copyFromLocal README.txt /</span><br><span class="line">-copyToLocal:从HDFS拷贝到本地</span><br><span class="line">hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br><span class="line">-<span class="built_in">cp</span></span><br><span class="line">hadoop fs -<span class="built_in">cp</span> /sanguo/shuguo/kongming.txt /zhuge.txt</span><br><span class="line">-get:等同于copyToLocal</span><br><span class="line">-getmerge:合并下载多个文件</span><br><span class="line">hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt</span><br><span class="line">-<span class="built_in">tail</span>:显示一个文件的末尾</span><br><span class="line">hadoop fs -<span class="built_in">tail</span> /sanguo/shuguo/kongming.txt</span><br><span class="line">-<span class="built_in">rm</span></span><br><span class="line">hadoop fs -<span class="built_in">rm</span> /user/atguigu/test/jinlian2.txt</span><br><span class="line">-<span class="built_in">rmdir</span>:删除空目录</span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /test</span><br><span class="line">-<span class="built_in">du</span>:统计文件夹的大小</span><br><span class="line">hadoop fs -<span class="built_in">du</span> -s -h /user/atguigu/test</span><br><span class="line">hadoop fs -<span class="built_in">du</span> -h /user/atguigu/test</span><br><span class="line">-setrep:设置HDFS中文件的副本数,只记在NameNode的元数据中，</span><br><span class="line">    当DataNode到达10台时，副本数才能到10</span><br><span class="line">hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h2 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h2><p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%205.png" alt="Untitled"></p>
<ol>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传。</li>
<li>客户端请求第一个 Block上传到哪几个DataNode服务器上。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h2 id="节点距离计算"><a href="#节点距离计算" class="headerlink" title="节点距离计算"></a>节点距离计算</h2><p>写数据时，NameNode会选择待上传数据最近距离的DataNode</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和</p>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%206.png" alt="Untitled"></p>
<h2 id="机架感知-副本存储节点选择"><a href="#机架感知-副本存储节点选择" class="headerlink" title="机架感知-副本存储节点选择"></a>机架感知-副本存储节点选择</h2><p>如果设定的是三副本</p>
<p>第一个副本在Client所处的节点上，若客户端在集群外，随机选一个</p>
<p>第二个副本和第一个副本位于相同几家，随机节点</p>
<p>第三个副本位于不同机架，随机节点</p>
<h2 id="读数据流程"><a href="#读数据流程" class="headerlink" title="读数据流程"></a>读数据流程</h2><p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%207.png" alt="Untitled"></p>
<p>1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。<br>2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br>3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。<br>4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<h2 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h2><p>FsImage:在磁盘中备份元数据 </p>
<p>Edits:只进行追加操作，每当元数据更新或添加时，修改内存中的元数据并 添加到Edits中。</p>
<p>(只有Edits和FsImage合并，生成最新FsImage，才代表完整的元数据信息)</p>
<p>Edits数据过大→定期进行FaImage和Edits合并→SecondaryNameNode专门用于Edits和FsImage合并(在NameNode上合并影响NameNode效率)</p>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%208.png" alt="Untitled"></p>
<ol>
<li>第一阶段：NameNode启动</li>
</ol>
<p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode记录操作日志，更新滚动日志。</p>
<p>（4）NameNode在内存中对数据进行增删改。</p>
<ol>
<li>第二阶段：Secondary NameNode工作</li>
</ol>
<p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</p>
<p>（2）Secondary NameNode请求执行CheckPoint。</p>
<p>（3）NameNode滚动正在写的Edits日志。</p>
<p>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p>
<p>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p>
<p>（6）生成新的镜像文件fsimage.chkpoint。</p>
<p>（7）拷贝fsimage.chkpoint到NameNode。</p>
<p>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<p><strong>NN和2NN工作机制详解：</strong></p>
<p>Fsimage：NameNode内存中元数据序列化后形成的文件。</p>
<p>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
<p>NameNode格式化后:</p>
<p>&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name&#x2F;current内文件:</p>
<p>fsimage_0000000000000</p>
<p>fsimage_0000000000000.md5</p>
<p>seen_txid</p>
<p>VERSION</p>
<ul>
<li>Fsimage:HDFS文件系统元数据的一个永久性的检查点，包括所有目录和文件inode的序列化信息</li>
<li>Edits:存放HDFS文件系统的所有的更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中</li>
<li>seen_txid中保存的是一个数字，就是最后一个edits_的数字</li>
<li>每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作</li>
</ul>
<h3 id="查看Fsimage和Edits文件内容"><a href="#查看Fsimage和Edits文件内容" class="headerlink" title="查看Fsimage和Edits文件内容"></a>查看Fsimage和Edits文件内容</h3><p>Fsimage</p>
<p>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</p>
<p>Edits</p>
<p>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径</p>
<h3 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h3><p>通常，SecondaryNameNode每隔一小时执行一次</p>
<p>hdfs-default.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>一分钟检查一次操作数，当操作数达到一百万时，SecondaryNameNode执行一次</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure>

<h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p>两种方式</p>
<p>一、<strong>将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</strong></p>
<ol>
<li>kill-9 NameNode进程</li>
<li>删除NameNode存储的数据（&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name）</li>
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
<li>重新启动NameNode</li>
</ol>
<p>二、<strong>使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</strong></p>
<ol>
<li>修改hdfs-site.xml中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol>
<li>kill -9 NameNode进程</li>
<li>删除NameNode存储的数据（&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name）</li>
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li>
<li>导入检查点数据（等待一会ctrl+c结束掉）</li>
<li>启动NameNode</li>
</ol>
<h2 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h2><ol>
<li>NameNode启动时，将镜像文件Fsimage载入内存，执行编辑日志Edits中各项操作，之后创建一个新的Fsimage文件和一个空的编辑日志。然后NameNode开始监听DataNode请求。此过程期间，NameNode运行在安全模式,即NameNode的文件系统对于客户端来说是只读的</li>
<li>DataNode启动，系统中数据块不由NameNode维护，而是以块列表形式存储在DataNode中。系统正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统</li>
<li>安全模式退出判断<br>若满足”最小副本条件”,NameNode会在30s之后就退出安全模式。最小副本条件:整个文件系统中99.9%的块满足最小副本级别(默认值:dfs.replication.min&#x3D;1).在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式</li>
</ol>
<p>语法:</p>
<p>查看安全模式状态:bin&#x2F;hdfs dfsadmin -safemode get</p>
<p>进入安全模式状态:bin&#x2F;hdfs dfsadmin -safemode enter</p>
<p>离开安全模式状态:bin&#x2F;hdfs dfsadmin -safemode leave</p>
<p>等待安全模式状态:bin&#x2F;hdfs dfsadmin -safemode wait</p>
<h2 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h2><p>本地目录配置多个，且每个目录存放内容相同</p>
<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%209.png" alt="Untitled"></p>
<p>1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。<br>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。<br>3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。<br>4）集群运行中可以安全加入和退出一些机器。</p>
<h3 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h3><p>DataNode节点保证数据完整性的方法:</p>
<p>1）当DataNode读取Block的时候，它会计算CheckSum。<br>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。<br>3）Client读取其他DataNode上的Block。<br>4）DataNode在其文件创建后周期验证CheckSum</p>
<p>crc</p>
<h3 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h3><p>DataNode进程死亡或网络故障无法与NamaNode通信时，经过一段时间才会判定死亡，称为超时时长。</p>
<p>默认10min+30s</p>
<p>超时时间TimeOut&#x3D;2<em>dfs.namenode.heartbeat.recheck-interval+10</em>dfs.heartbeat.interval</p>
<p>默认dfs.namenode.heartbeat.recheck-interval&#x3D;5min,单位为毫秒</p>
<p>dfs.heartbeat.interval&#x3D;3s，单位为秒</p>
<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="服役新数据节点"><a href="#服役新数据节点" class="headerlink" title="服役新数据节点"></a>服役新数据节点</h2><p>hadoop104克隆一台新主机hadoop105</p>
<p>修改IP地址和主机名称</p>
<p>删除原来HDFS文件系统留存的文件(&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data和log)</p>
<p>source &#x2F;etc&#x2F;profile</p>
<p>直接启动DataNode，即可关联到集群</p>
<p>sbin&#x2F;hadoop-daemon.sh start datanode</p>
<p>sbin&#x2F;yarn-daemon.sh start nodemanager</p>
<p>若数据不均衡，执行</p>
<p>[atguigu@hadoop102 sbin]$ cd sbin;.&#x2F;start-balancer.sh</p>
<h2 id="添加白名单"><a href="#添加白名单" class="headerlink" title="添加白名单"></a>添加白名单</h2><p>不在白名单的主机节点都会被退出</p>
<ol>
<li>在NameNode的&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop目录下创建dfs.hosts文件</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ touch dfs.hosts</span><br><span class="line"># 不添加hadoop105</span><br><span class="line">vim dfs.hosts</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<ol>
<li>hdfs-site.xml</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol>
<li>xsync hdfs-site.xml</li>
<li>刷新NameNode hdfs dfsadmin -refreshNodes</li>
<li>更新ResourceManager节点 yarn rmadmin -refreshNodes</li>
<li>web页面查看发现没有hadoop105</li>
<li>如果数据不均衡，可以用命令实现集群的再平衡 .&#x2F;start-balancer.sh</li>
</ol>
<h3 id="黑名单退役"><a href="#黑名单退役" class="headerlink" title="黑名单退役"></a>黑名单退役</h3><p>在黑名单上面的主机都会被强制退出</p>
<ol>
<li>在NameNode的&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop目录下创建dfs.hosts.exclude文件</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[atguigu@hadoop102 hadoop]$ touch dfs.hosts.exclude</span><br><span class="line"># 要退役的节点</span><br><span class="line">vi dfs.hosts.exclude</span><br><span class="line">hadoop105</span><br></pre></td></tr></table></figure>

<ol>
<li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol>
<li>刷新NameNode、刷新ResourceManager</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<ol>
<li>检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点</li>
<li>等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure>

<ol>
<li>如果数据不均衡，可以用命令实现集群的再平衡</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>

<p>不允许白名单和黑名单中同时出现同一个主机名称。</p>
<h3 id="DataNode多目录配置"><a href="#DataNode多目录配置" class="headerlink" title="DataNode多目录配置"></a>DataNode多目录配置</h3><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</p>
<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="HDFS2-X新特性"><a href="#HDFS2-X新特性" class="headerlink" title="HDFS2.X新特性"></a>HDFS2.X新特性</h2><p>集群间数据拷贝</p>
<p>scp:<br>scp -r hello.txt <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt</a>  &#x2F;&#x2F; 推 push<br>scp -r <a href="mailto:root@hadoop103:/user/atguigu/hello.txt%20%20hello.txt">root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt  hello.txt</a>  &#x2F;&#x2F; 拉 pull<br>scp -r <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:&#x2F;user&#x2F;atguigu&#x2F;hello.txt</a> root@hadoop104:&#x2F;user&#x2F;atguigu   &#x2F;&#x2F;是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</p>
<p>采用distcp命令实现两个Hadoop集群之间的递归数据复制:</p>
<p>bin&#x2F;hadoop distcp hdfs:&#x2F;&#x2F;haoop102:9000&#x2F;user&#x2F;atguigu&#x2F;hello.txt hdfs:&#x2F;&#x2F;hadoop103:9000&#x2F;user&#x2F;atguigu&#x2F;hello.txt</p>
<h3 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h3><p>小文件所需磁盘容量与数据块的大小无关。如1MB文件使用1MB磁盘空间，不是128M</p>
<p>解决办法：HDFS存档文件或HAR文件</p>
<p>bin&#x2F;hadoop archive -archiveName input.har –p &#x2F;user&#x2F;atguigu&#x2F;input &#x2F;user&#x2F;atguigu&#x2F;output</p>
<p>查看归档：</p>
<p>hadoop fs -lsr &#x2F;user&#x2F;atguigu&#x2F;output&#x2F;input.har</p>
<p>hadoop fs -lsr har:&#x2F;&#x2F;&#x2F;user&#x2F;atguigu&#x2F;output&#x2F;input.har</p>
<p>解归档文件：</p>
<p>hadoop fs -cp har:&#x2F;&#x2F;&#x2F; user&#x2F;atguigu&#x2F;output&#x2F;input.har&#x2F;* &#x2F;user&#x2F;atguigu</p>
<h2 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h2><p>默认值：</p>
<p>fs.trash.interval&#x3D;0,0表示禁用回收站，其他值表示设置文件的存活时间</p>
<p>fs.trash.checkpoint.interval&#x3D;0,检查回收站的间隔时间。如果该值为0，则和fs.trash.interval的参数值相等</p>
<p>要求fs.trash.checkpoint.interval≤fs.trash.interval</p>
<p>启用回收站：</p>
<p>core-site.xml,配置为1分钟</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>回收站路径:</p>
<p>&#x2F;user&#x2F;atguigu&#x2F;.Trash&#x2F;…</p>
<p>修改访问垃圾回收站用户名称：</p>
<p>默认dr.who</p>
<p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Trash</span> <span class="variable">trash</span> <span class="operator">=</span> New <span class="title function_">Trash</span><span class="params">(conf)</span>;</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure>

<p>恢复回收站数据:</p>
<p>hadoop fs -mv<br>&#x2F;user&#x2F;atguigu&#x2F;.Trash&#x2F;Current&#x2F;user&#x2F;atguigu&#x2F;input    &#x2F;user&#x2F;atguigu&#x2F;input</p>
<p>清空回收站：hadoop fs -expunge</p>
<h3 id="快照管理"><a href="#快照管理" class="headerlink" title="快照管理"></a>快照管理</h3><p>开启指定目录的快照功能:hdfs dfsadmin -allowSnapshot 路径</p>
<p>禁用指定目录的快照功能，默认是禁用：hdfs dfsadmin -disallowSnapshot 路径</p>
<p>对目录创建快照：hdfs dfs -creteSnapshot 路径</p>
<p>指定名称创建快照：hdfs dfs -createSnapshot 路径 名称</p>
<p>重命名快照：hdfs dfs -renameSnapshot 路径 旧名称 新名称</p>
<p>列出当前用户所有可快照目录：hdfs lsSnapshottableDir</p>
<p>比较两个快照目录的不同之处:hdfs snapshotDiff 路径1 路径2</p>
<p>删除快照：hdfs dfs -deleteSnapshot <path></path> <snapshotName></snapshotName></p>
<h2 id="HDFS-HA"><a href="#HDFS-HA" class="headerlink" title="HDFS HA"></a>HDFS HA</h2><p>High Available,即高可用(7*24小时不中断服务)</p>
<p>分为HDFS的HA和YARN的HA</p>
<p>若NameNode故障或升级，将导致集群无法使用</p>
<p>HDFS HA通过配置Active&#x2F;Standby两个NameNodes实现对NameNode的热备来解决上述问题</p>
<p>工作机制：双NameNode消除单点故障</p>
<p>工作要点：</p>
<ol>
<li>元数据管理方式需要改变</li>
</ol>
<p>内存中各自保存一份元数据；</p>
<p>Edits日志只有Active状态的NameNode节点可以做写操作；</p>
<p>两个NameNode都可以读取Edits；</p>
<p>共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</p>
<ol>
<li>需要一个状态管理功能模块</li>
</ol>
<p>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</p>
<ol>
<li>必须保证两个NameNode之间能够ssh无密码登录</li>
<li>隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</li>
</ol>
<p>HA自动故障转移需用到:ZooKeeper和ZKFailoverController（ZKFC）进程</p>
<p>Zookeeper的作用:</p>
<p><strong>1）故障检测：</strong>集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</p>
<p><strong>2）现役NameNode选择：</strong>ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</p>
<p>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：</p>
<p><strong>1）健康监测：</strong>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</p>
<p><strong>2）ZooKeeper会话管理：</strong>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</p>
<p><strong>3）基于ZooKeeper的选择：</strong>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为Active状态。</p>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%2010.png" alt="Untitled"></p>
<p>集群配置：</p>
<p>规划：</p>
<table>
<thead>
<tr>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>NameNode</td>
<td></td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>ZK</td>
<td>ZK</td>
<td>ZK</td>
</tr>
<tr>
<td></td>
<td>ResourceManager</td>
<td></td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<ol>
<li>zookeeper安装</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line"><span class="built_in">mkdir</span> ha</span><br><span class="line"><span class="built_in">cd</span> /opt/module</span><br><span class="line"><span class="built_in">cp</span> -r hadoop-2.7.2/ /opt/ha/</span><br><span class="line"><span class="built_in">cd</span> /opt/ha/hadoop-2.7.2/etc/hadoop</span><br><span class="line">vim hadoop-env.sh</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">vim core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">			&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        	&lt;value&gt;hdfs://mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">			&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">			&lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- 完全分布式集群名称 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mycluster&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- nn1的RPC通信地址 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop102:9000&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- nn2的RPC通信地址 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop103:9000&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- nn1的http通信地址 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop102:50070&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- nn2的http通信地址 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop103:50070&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;/home/atguigu/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 声明journalnode服务器存储目录--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 关闭权限检查--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.permissions.enable&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">  		&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">xsync.sh /opt/ha</span><br><span class="line"></span><br><span class="line"><span class="comment"># 各journalnode节点启动journalnode</span></span><br><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br><span class="line"><span class="comment"># 在[nn1]上，对其进行格式化，并启动</span></span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="comment"># 在[nn2]上，同步nn1的元数据信息</span></span><br><span class="line">bin/hdfs namenode -bootstrapStandby</span><br><span class="line"><span class="comment"># 启动[nn2]</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="comment"># 在[nn1]上，启动所有datanode</span></span><br><span class="line">sbin/hadoop-daemons.sh start datanode</span><br><span class="line"><span class="comment"># 将[nn1]切换为Active</span></span><br><span class="line">bin/hdfs haadmin -transitionToActive nn1</span><br><span class="line"><span class="comment"># 查看是否Active</span></span><br><span class="line">bin/hdfs haadmin -getServiceState nn1</span><br><span class="line"><span class="comment"># 配置HDFS-HA自动故障转移</span></span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭所有HDFS服务</span></span><br><span class="line">sbin/stop-dfs.sh</span><br><span class="line"><span class="comment"># 启动Zookeeper集群</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="comment"># 初始化HA在Zookeeper中状态</span></span><br><span class="line">bin/hdfs zkfc -formatZK</span><br><span class="line"><span class="comment"># 启动HDFS服务</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="comment"># 在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode</span></span><br><span class="line">sbin/hadoop-daemin.sh start zkfc</span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line"><span class="comment"># 将Active NameNode进程kill</span></span><br><span class="line"><span class="built_in">kill</span> -9 namenode的进程<span class="built_in">id</span></span><br><span class="line"><span class="comment"># 将Active NameNode机器断开网络</span></span><br><span class="line">service network stop</span><br></pre></td></tr></table></figure>

<h2 id="YARN-HA"><a href="#YARN-HA" class="headerlink" title="YARN-HA"></a>YARN-HA</h2><h3 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h3><p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%2011.png" alt="Untitled"></p>
<h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><table>
<thead>
<tr>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>NameNode</td>
<td></td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>ZK</td>
<td>ZK</td>
<td>ZK</td>
</tr>
<tr>
<td>ResourceManager</td>
<td>ResourceManager</td>
<td></td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<p>yarn-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--启用resourcemanager ha--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">    &lt;!--声明两台resourcemanager的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cluster-yarn1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">    &lt;!--指定zookeeper集群的地址--&gt; </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--启用自动恢复--&gt; </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">    &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;     &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>分发到其他节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动hdfs</span></span><br><span class="line"><span class="comment"># 在各个JournalNode节点上，输入以下命令启动journalnode服务</span></span><br><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br><span class="line"><span class="comment"># 在[nn1]上，对其进行格式化，并启动</span></span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="comment"># 在[nn2]上，同步nn1的元数据信息</span></span><br><span class="line">bin/hdfs namenode -bootstrapStandby</span><br><span class="line"><span class="comment"># 启动[nn2]</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="comment"># 启动所有DataNode</span></span><br><span class="line">sbin/hadoop-daemons.sh start datanode</span><br><span class="line"><span class="comment"># 将[nn1]切换为Active</span></span><br><span class="line">bin/hdfs haadmin -transitionToActive nn1</span><br><span class="line"><span class="comment"># 启动YARN</span></span><br><span class="line"><span class="comment"># 在hadoop102中执行</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 在hadoop103中执行</span></span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="comment"># 查看服务状态</span></span><br><span class="line">bin/yarn rmadmin -getServiceState rm1</span><br></pre></td></tr></table></figure>

<h2 id="HDFS-Federation架构设计"><a href="#HDFS-Federation架构设计" class="headerlink" title="HDFS Federation架构设计"></a>HDFS Federation架构设计</h2><p>NameNode架构的局限性：</p>
<ol>
<li>Namespace（命名空间）的限制<br>由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB</li>
<li>隔离问题<br>由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</li>
<li>性能的瓶颈<br>由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量</li>
</ol>
<p>架构设计：</p>
<table>
<thead>
<tr>
<th>NameNode</th>
<th>NameNode</th>
<th>NameNode</th>
</tr>
</thead>
<tbody><tr>
<td>元数据</td>
<td>元数据</td>
<td>元数据</td>
</tr>
<tr>
<td>Log</td>
<td>machine</td>
<td>电商数据&#x2F;话单数据</td>
</tr>
</tbody></table>
<p><img src="/2022/04/27/hadoop-hdfs/hdfs%20a8b14862e46046f8abe7ff78f1f162b9/Untitled%2012.png" alt="Untitled"></p>
<p>应用思考:不同应用可以使用不同NameNode进行数据管理</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">songjj</p>
  <div class="site-description" itemprop="description">编程学习之旅</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">songjj</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  

</body>
</html>
